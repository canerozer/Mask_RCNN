{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import datetime\n",
    "import os\n",
    "from pycocotools import coco\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CATEGORY INFORMATION\n",
    "### !!! NEED HELP FOR MATCHING THE CLASSES IN VOT2016 TO THEIR CORRESPONDING ONES IN MS-COCO !!! ### \n",
    "### Legend ###\n",
    "### * --> Will be defined into Mask R-CNN as a new category\n",
    "### ! --> Might be some issues with the assignment, they have to be rechecked\n",
    "dict_of_categories = {'face': 91}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'licenses', 'info', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dir = \"Datasets/coco/annotations_old/instances_train2014.json\"\n",
    "\n",
    "# For viewing the content of the new json file.\n",
    "# json_dir = \"Datasets/coco/annotations/instances_train2014.json\"\n",
    "\n",
    "# For debugging, then please restore it.\n",
    "# json_dir = \"Datasets/coco/annotations_old/instances_valminusminival2014.json\"\n",
    "\n",
    "\n",
    "minival2014 = json.load(open(json_dir))\n",
    "\n",
    "minival2014.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minival2014['images'] = []\n",
    "# minival2014['annotations'] = []\n",
    "# minival2014['images'].append({'date_captured': '2013-11-14 11:18:45', 'url': 'http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg', 'id': 391895, 'height': 360, 'width': 640, 'file_name': 'COCO_val2014_000000391895.jpg', 'license': 3})\n",
    "# minival2014['annotations'].append({'category_id': 18, 'image_id': 42, 'id': 1817255, 'bbox': [214.15, 41.29, 348.26, 243.78], 'area': 53481.5118, 'segmentation': [[382.48, 268.63, 330.24, 229.93, 278.97, 205.75, 228.66, 143.83, 214.15, 140.93, 225.76, 134.16, 257.69, 123.52, 277.03, 82.89, 328.3, 48.06, 433.75, 41.29, 502.43, 79.99, 561.44, 168.02, 547.9, 216.39, 562.41, 246.38, 542.09, 285.07, 510.17, 285.07, 467.61, 223.16, 419.24, 253.15, 394.09, 264.76]], 'iscrowd': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = minival2014['annotations'][604864]\n",
    "# print(list['segmentation'])\n",
    "# for d, i in enumerate(list):\n",
    "#     if i['iscrowd']:\n",
    "#         print(d)\n",
    "#         print(i['segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the classes, view the category list at first!!\n",
    "minival2014['categories'].append({'id': 91, 'name': 'face', 'supercategory': 'person'})\n",
    "# minival2014['categories'].append({'id': 92, 'name': 'fish', 'supercategory': 'animal'})\n",
    "# minival2014['categories']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the added class at the bottom.\n",
    "# minival2014['images'].pop()\n",
    "# (minival2014['images'][-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to be used:\n",
    "def isjpg(string):\n",
    "    if string[-4:]==\".jpg\":\n",
    "        return True \n",
    "    \n",
    "def ispng(string):\n",
    "    if string[-4:]==\".png\":\n",
    "        return True \n",
    "    \n",
    "def DateCaptured():\n",
    "    dt = str(datetime.datetime.now())\n",
    "    date, time = dt.split()\n",
    "    time = time.split(\".\")[0]\n",
    "    return date+\" \"+time\n",
    "\n",
    "def coco_bbox_creator(x, y):\n",
    "    x = list(map(lambda x: float(x), x))\n",
    "    y = list(map(lambda x: float(x), y))\n",
    "    x_min = min(x)\n",
    "    y_min = min(y)\n",
    "    w = max(x) - x_min\n",
    "    h = max(y) - y_min\n",
    "    return [x_min, y_min, w, h]\n",
    "\n",
    "def PolyArea(x,y):\n",
    "    \"\"\"\n",
    "    Parameters:   x, y: The inputs are representing the nd-array of coordinates\n",
    "    of a polygon.\n",
    "    \n",
    "    Returns: The area of the polygon\n",
    "    \"\"\"\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def xml_retreiver(tree_object, key_name):\n",
    "    for iterator in tree_object.iter(key_name):\n",
    "        return iterator.text\n",
    "    \n",
    "def iou_calculator(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Parameters:   bbox1, bbox2: list or numpy array of bounding box coordinates.\n",
    "    The input should contain the top-left corner's x and y coordinates and \n",
    "    width and height of the bounding boxes.\n",
    "    \n",
    "    Assertations: width and height informations of bbox1 and bbox2 should be \n",
    "    larger than 0.\n",
    "    \n",
    "    Returns:      iou: A floating point decimal representing the IoU ratio, which\n",
    "    is the division of bounding box areas of intersection to their union.\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    assert w1 and w2 > 0\n",
    "    assert w1 and h2 > 0\n",
    "    \n",
    "    iou = 0\n",
    "    if (((x1>x2 and x1<x2+w2) or (x1+w1>x2 and x1+w1<x2+w2) or \n",
    "        (x2>x1 and x2<x1+w1) or (x2+w2>x1 and x2+w2<x1+w1)) and \n",
    "        ((y1>y2 and y1<y2+h2) or (y1+h1>y2 and y1+h1<y2+h2) or\n",
    "        (y2>y1 and y2<y1+h1) or (y2+h2>y1 and y2+h2<y1+h1))):\n",
    "        iou_xmin = float(max(x1, x2))\n",
    "        iou_xmax = float(min(x1+w1, x2+w2))\n",
    "        iou_ymin = float(max(y1, y2))\n",
    "        iou_ymax = float(min(y1+h1, y2+h2))\n",
    "        intersection_area = (iou_ymax - iou_ymin)*(iou_xmax - iou_xmin)\n",
    "        total_area = float(w1)*float(h1) + float(w2)*float(h2) - intersection_area\n",
    "        iou = intersection_area/total_area\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "def txt_bbox_parser(input_location):\n",
    "    \"\"\"\n",
    "    Parameters: input_location: The input will be a text file denoting the bounding\n",
    "    boxes for every frame in such format:\n",
    "    frame_id, x1, y1, x2, y2, x3, y3, x4, y4\n",
    "    \n",
    "    Returns: image_nr: A list containing the the frame numbers. \n",
    "             xywh: A list containing the bounding boxes in (x1, y1, w, h) format,\n",
    "                   where w and h are the width and height of a bounding box respectively.\n",
    "    \"\"\"\n",
    "    with open(input_location) as f:\n",
    "        bboxes = f.readlines()\n",
    "    image_nr = []\n",
    "    xywh = []\n",
    "    for d, bbox in enumerate(bboxes):\n",
    "        image_nr.append(bbox.split(\",\")[0])\n",
    "        coords = np.array(bbox.split(\",\")[1:]).reshape((-1, 2))\n",
    "        x, y = coords[:,0], coords[:,1]\n",
    "        xywh.append(coco_bbox_creator(x, y))\n",
    "    return image_nr, xywh\n",
    "    \n",
    "import pycocotools._mask as _mask\n",
    "encode_mask = _mask.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000001', '000021', '000021', '000021', '000025'] [[125.0, 37.0, 150.0, 166.0], [92.0, 89.0, 39.0, 50.0], [233.0, 48.0, 49.0, 46.0], [61.0, 189.0, 48.0, 49.0], [232.0, 8.0, 56.0, 56.0]]\n",
      "00000001.jpg\n",
      "00000002.jpg\n",
      "00000003.jpg\n",
      "00000004.jpg\n",
      "00000005.jpg\n",
      "00000006.jpg\n",
      "00000007.jpg\n",
      "00000008.jpg\n",
      "00000009.jpg\n",
      "00000010.jpg\n",
      "00000011.jpg\n",
      "00000012.jpg\n",
      "00000013.jpg\n",
      "00000014.jpg\n",
      "00000015.jpg\n",
      "00000016.jpg\n",
      "00000017.jpg\n",
      "00000018.jpg\n",
      "00000019.jpg\n",
      "00000020.jpg\n",
      "00000021.jpg\n",
      "00000022.jpg\n",
      "00000023.jpg\n",
      "00000024.jpg\n",
      "00000025.jpg\n",
      "00000026.jpg\n",
      "00000027.jpg\n",
      "00000028.jpg\n",
      "00000029.jpg\n",
      "00000030.jpg\n",
      "00000031.jpg\n",
      "00000032.jpg\n",
      "00000033.jpg\n",
      "00000034.jpg\n",
      "00000035.jpg\n",
      "00000036.jpg\n",
      "00000037.jpg\n",
      "00000038.jpg\n",
      "00000039.jpg\n",
      "00000040.jpg\n",
      "00000041.jpg\n",
      "00000042.jpg\n",
      "00000043.jpg\n",
      "00000044.jpg\n",
      "00000045.jpg\n",
      "00000046.jpg\n",
      "00000047.jpg\n",
      "00000048.jpg\n",
      "00000049.jpg\n",
      "00000050.jpg\n",
      "00000051.jpg\n",
      "00000052.jpg\n",
      "00000053.jpg\n",
      "00000054.jpg\n",
      "00000055.jpg\n",
      "00000056.jpg\n",
      "00000057.jpg\n",
      "00000058.jpg\n",
      "00000059.jpg\n",
      "00000060.jpg\n",
      "00000061.jpg\n",
      "00000062.jpg\n",
      "00000063.jpg\n",
      "00000064.jpg\n",
      "00000065.jpg\n",
      "00000066.jpg\n",
      "00000067.jpg\n",
      "00000068.jpg\n",
      "00000069.jpg\n",
      "00000070.jpg\n",
      "00000071.jpg\n",
      "00000072.jpg\n",
      "00000073.jpg\n",
      "00000074.jpg\n",
      "00000075.jpg\n",
      "00000076.jpg\n",
      "00000077.jpg\n",
      "00000078.jpg\n",
      "00000079.jpg\n",
      "00000080.jpg\n",
      "00000081.jpg\n",
      "00000082.jpg\n",
      "00000083.jpg\n",
      "00000084.jpg\n",
      "00000085.jpg\n",
      "00000086.jpg\n",
      "00000087.jpg\n",
      "00000088.jpg\n",
      "00000089.jpg\n",
      "00000090.jpg\n",
      "00000091.jpg\n",
      "00000092.jpg\n",
      "00000093.jpg\n",
      "00000094.jpg\n",
      "00000095.jpg\n",
      "00000096.jpg\n",
      "00000097.jpg\n",
      "00000098.jpg\n",
      "00000099.jpg\n",
      "000001.jpg\n",
      "00000100.jpg\n",
      "00000101.jpg\n",
      "00000102.jpg\n",
      "00000103.jpg\n",
      "00000104.jpg\n",
      "00000105.jpg\n",
      "00000106.jpg\n",
      "00000107.jpg\n",
      "00000108.jpg\n",
      "00000109.jpg\n",
      "00000110.jpg\n",
      "00000111.jpg\n",
      "00000112.jpg\n",
      "00000113.jpg\n",
      "00000114.jpg\n",
      "00000115.jpg\n",
      "00000116.jpg\n",
      "00000117.jpg\n",
      "00000118.jpg\n",
      "00000119.jpg\n",
      "00000120.jpg\n",
      "00000124.jpg\n",
      "00000125.jpg\n",
      "00000126.jpg\n",
      "00000127.jpg\n",
      "00000128.jpg\n",
      "00000129.jpg\n",
      "00000130.jpg\n",
      "00000131.jpg\n",
      "00000132.jpg\n",
      "00000133.jpg\n",
      "00000134.jpg\n",
      "00000135.jpg\n",
      "00000136.jpg\n",
      "00000137.jpg\n",
      "00000138.jpg\n",
      "00000139.jpg\n",
      "00000140.jpg\n",
      "00000141.jpg\n",
      "00000142.jpg\n",
      "00000143.jpg\n",
      "00000144.jpg\n",
      "00000145.jpg\n",
      "00000146.jpg\n",
      "00000147.jpg\n",
      "00000148.jpg\n",
      "00000149.jpg\n",
      "00000150.jpg\n",
      "00000151.jpg\n",
      "00000152.jpg\n",
      "00000153.jpg\n",
      "00000154.jpg\n",
      "00000155.jpg\n",
      "00000156.jpg\n",
      "00000157.jpg\n",
      "00000158.jpg\n",
      "00000159.jpg\n",
      "00000160.jpg\n",
      "00000161.jpg\n",
      "00000162.jpg\n",
      "00000163.jpg\n",
      "00000164.jpg\n",
      "00000165.jpg\n",
      "00000166.jpg\n",
      "00000167.jpg\n",
      "00000168.jpg\n",
      "00000169.jpg\n",
      "00000170.jpg\n",
      "00000171.jpg\n",
      "00000172.jpg\n",
      "00000173.jpg\n",
      "00000174.jpg\n",
      "00000175.jpg\n",
      "00000176.jpg\n",
      "00000177.jpg\n",
      "00000178.jpg\n",
      "00000179.jpg\n",
      "00000180.jpg\n",
      "00000181.jpg\n",
      "00000182.jpg\n",
      "00000183.jpg\n",
      "00000184.jpg\n",
      "00000185.jpg\n",
      "00000186.jpg\n",
      "00000187.jpg\n",
      "00000188.jpg\n",
      "00000189.jpg\n",
      "00000190.jpg\n",
      "00000191.jpg\n",
      "00000192.jpg\n",
      "00000193.jpg\n",
      "00000194.jpg\n",
      "00000195.jpg\n",
      "00000196.jpg\n",
      "00000197.jpg\n",
      "00000198.jpg\n",
      "00000199.jpg\n",
      "00000200.jpg\n",
      "00000201.jpg\n",
      "00000202.jpg\n",
      "00000203.jpg\n",
      "00000204.jpg\n",
      "00000205.jpg\n",
      "00000206.jpg\n",
      "00000207.jpg\n",
      "00000208.jpg\n",
      "00000209.jpg\n",
      "00000210.jpg\n",
      "00000211.jpg\n",
      "00000212.jpg\n",
      "00000213.jpg\n",
      "00000214.jpg\n",
      "00000215.jpg\n",
      "00000216.jpg\n",
      "00000217.jpg\n",
      "00000218.jpg\n",
      "00000219.jpg\n",
      "00000220.jpg\n",
      "00000221.jpg\n",
      "00000222.jpg\n",
      "00000223.jpg\n",
      "00000224.jpg\n",
      "00000225.jpg\n",
      "00000226.jpg\n",
      "00000227.jpg\n",
      "00000228.jpg\n",
      "00000229.jpg\n",
      "00000230.jpg\n",
      "00000231.jpg\n",
      "00000232.jpg\n",
      "00000233.jpg\n",
      "00000234.jpg\n",
      "00000235.jpg\n",
      "00000236.jpg\n",
      "00000237.jpg\n",
      "00000238.jpg\n",
      "00000239.jpg\n",
      "00000240.jpg\n",
      "00000241.jpg\n",
      "00000242.jpg\n",
      "00000243.jpg\n",
      "00000244.jpg\n",
      "00000245.jpg\n",
      "00000246.jpg\n",
      "00000247.jpg\n",
      "00000248.jpg\n",
      "00000249.jpg\n",
      "00000250.jpg\n",
      "00000251.jpg\n",
      "00000252.jpg\n",
      "00000253.jpg\n",
      "00000254.jpg\n",
      "00000255.jpg\n",
      "00000256.jpg\n",
      "00000257.jpg\n",
      "00000258.jpg\n",
      "00000259.jpg\n",
      "00000260.jpg\n",
      "00000261.jpg\n",
      "00000262.jpg\n",
      "00000263.jpg\n",
      "00000264.jpg\n",
      "00000265.jpg\n",
      "00000266.jpg\n",
      "00000267.jpg\n",
      "00000268.jpg\n",
      "00000269.jpg\n",
      "00000270.jpg\n",
      "00000271.jpg\n",
      "00000272.jpg\n",
      "00000273.jpg\n",
      "00000274.jpg\n",
      "00000275.jpg\n",
      "00000276.jpg\n",
      "00000277.jpg\n",
      "00000278.jpg\n",
      "00000279.jpg\n",
      "00000280.jpg\n",
      "00000281.jpg\n",
      "00000282.jpg\n",
      "00000283.jpg\n",
      "00000284.jpg\n",
      "00000285.jpg\n",
      "00000286.jpg\n",
      "00000287.jpg\n",
      "00000288.jpg\n",
      "00000289.jpg\n",
      "00000290.jpg\n",
      "00000291.jpg\n",
      "00000292.jpg\n",
      "00000293.jpg\n",
      "00000294.jpg\n",
      "00000295.jpg\n",
      "00000296.jpg\n",
      "00000297.jpg\n",
      "00000298.jpg\n",
      "00000299.jpg\n",
      "00000300.jpg\n",
      "00000301.jpg\n",
      "00000302.jpg\n",
      "00000303.jpg\n",
      "00000304.jpg\n",
      "00000305.jpg\n",
      "00000306.jpg\n",
      "00000307.jpg\n",
      "00000308.jpg\n",
      "00000309.jpg\n",
      "00000310.jpg\n",
      "00000311.jpg\n",
      "00000312.jpg\n",
      "00000313.jpg\n",
      "00000314.jpg\n",
      "00000315.jpg\n",
      "00000316.jpg\n",
      "00000317.jpg\n",
      "00000318.jpg\n",
      "00000319.jpg\n",
      "00000320.jpg\n",
      "000021.jpg\n",
      "000025.jpg\n",
      "000027.jpg\n",
      "000030.jpg\n",
      "000035.jpg\n",
      "000038.jpg\n",
      "000043.jpg\n",
      "000050.jpg\n",
      "000051.jpg\n",
      "000058.jpg\n",
      "000069.jpg\n",
      "000073.jpg\n",
      "000076.jpg\n",
      "000081.jpg\n",
      "000085.jpg\n",
      "000089.jpg\n",
      "000090.jpg\n",
      "000096.jpg\n",
      "000101.jpg\n",
      "000104.jpg\n",
      "000105.jpg\n",
      "000113.jpg\n",
      "000124.jpg\n",
      "000125.jpg\n",
      "000127.jpg\n",
      "000129.jpg\n",
      "000133.jpg\n",
      "000138.jpg\n",
      "000139.jpg\n",
      "000144.jpg\n",
      "000146.jpg\n",
      "000151.jpg\n",
      "000159.jpg\n",
      "000162.jpg\n",
      "000164.jpg\n",
      "000165.jpg\n",
      "000166.jpg\n",
      "000169.jpg\n",
      "000170.jpg\n",
      "000171.jpg\n",
      "000173.jpg\n",
      "000174.jpg\n",
      "000177.jpg\n",
      "000181.jpg\n",
      "000182.jpg\n",
      "000191.jpg\n",
      "000192.jpg\n",
      "000193.jpg\n",
      "000200.jpg\n",
      "000202.jpg\n",
      "000205.jpg\n",
      "000206.jpg\n",
      "000212.jpg\n",
      "000220.jpg\n",
      "000222.jpg\n",
      "000229.jpg\n",
      "000230.jpg\n",
      "000231.jpg\n",
      "000237.jpg\n",
      "000245.jpg\n",
      "000247.jpg\n",
      "000248.jpg\n",
      "000252.jpg\n",
      "000257.jpg\n",
      "000258.jpg\n",
      "000259.jpg\n",
      "000264.jpg\n",
      "000265.jpg\n",
      "000269.jpg\n",
      "000271.jpg\n",
      "000272.jpg\n",
      "000275.jpg\n",
      "000276.jpg\n",
      "000278.jpg\n",
      "000280.jpg\n",
      "000282.jpg\n",
      "000283.jpg\n",
      "000285.jpg\n",
      "000286.jpg\n",
      "000287.jpg\n",
      "000297.jpg\n",
      "000298.jpg\n",
      "000299.jpg\n",
      "000302.jpg\n",
      "000305.jpg\n",
      "000308.jpg\n",
      "000310.jpg\n",
      "000315.jpg\n",
      "000319.jpg\n",
      "000320.jpg\n",
      "000321.jpg\n",
      "000322.jpg\n",
      "000323.jpg\n",
      "000328.jpg\n",
      "000331.jpg\n",
      "000337.jpg\n",
      "000339.jpg\n",
      "000342.jpg\n",
      "000346.jpg\n",
      "000348.jpg\n",
      "000352.jpg\n",
      "000356.jpg\n",
      "000359.jpg\n",
      "000367.jpg\n",
      "000368.jpg\n",
      "000369.jpg\n",
      "000372.jpg\n",
      "000374.jpg\n",
      "000377.jpg\n",
      "000378.jpg\n",
      "000386.jpg\n",
      "000388.jpg\n",
      "000392.jpg\n",
      "000393.jpg\n",
      "000394.jpg\n",
      "000405.jpg\n",
      "000407.jpg\n",
      "000409.jpg\n",
      "000413.jpg\n",
      "000414.jpg\n",
      "000423.jpg\n",
      "000428.jpg\n",
      "000433.jpg\n",
      "000434.jpg\n",
      "000438.jpg\n",
      "000446.jpg\n",
      "000448.jpg\n",
      "000449.jpg\n",
      "000453.jpg\n",
      "000457.jpg\n",
      "000468.jpg\n",
      "000476.jpg\n",
      "000477.jpg\n",
      "000479.jpg\n",
      "000483.jpg\n",
      "000485.jpg\n",
      "000490.jpg\n",
      "000493.jpg\n",
      "000497.jpg\n",
      "000498.jpg\n",
      "000499.jpg\n",
      "000500.jpg\n",
      "000502.jpg\n",
      "000506.jpg\n",
      "000507.jpg\n",
      "000516.jpg\n",
      "000517.jpg\n",
      "000520.jpg\n",
      "000523.jpg\n",
      "000524.jpg\n",
      "000526.jpg\n",
      "000530.jpg\n",
      "000531.jpg\n",
      "000534.jpg\n",
      "000535.jpg\n",
      "000539.jpg\n",
      "000545.jpg\n",
      "000546.jpg\n",
      "000555.jpg\n",
      "000562.jpg\n",
      "000566.jpg\n",
      "000567.jpg\n",
      "000578.jpg\n",
      "000586.jpg\n",
      "000587.jpg\n",
      "000589.jpg\n",
      "000594.jpg\n",
      "000602.jpg\n",
      "000604.jpg\n",
      "000606.jpg\n",
      "000612.jpg\n",
      "000613.jpg\n",
      "000615.jpg\n",
      "000616.jpg\n",
      "000617.jpg\n",
      "000624.jpg\n",
      "000633.jpg\n",
      "000639.jpg\n",
      "000641.jpg\n",
      "000642.jpg\n",
      "000643.jpg\n",
      "000644.jpg\n",
      "000652.jpg\n",
      "000662.jpg\n",
      "000664.jpg\n",
      "000670.jpg\n",
      "000677.jpg\n",
      "000683.jpg\n"
     ]
    }
   ],
   "source": [
    "# LabelMe type\n",
    "dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "dataset_name = \"MSPR_Dataset/\"\n",
    "annot_dir = \"Annotations/\"\n",
    "mask_dir = \"Masks/\"\n",
    "image_dir = \"Images/\"\n",
    "\n",
    "target_dataset_name = os.path.join(dataset_dir, \"coco/train2014/\")\n",
    "\n",
    "##########################\n",
    "# ANNOTATIONS OF FILIZ   #\n",
    "##########################\n",
    "filiz_annotations = os.path.join(\"/home/mspr/Desktop/Mask_RCNN/Datasets/MSPR_Dataset/face/filiz_Annotations\",\n",
    "                                 \"pascal_voc_face annotation.txt\")\n",
    "\n",
    "image_nrs, bbox_filiz = txt_bbox_parser(filiz_annotations)\n",
    "\n",
    "avg_iou = []\n",
    "\n",
    "# Added image and annotation ids will start from 600k. \n",
    "image_id = 600000\n",
    "annotation_id = 600000\n",
    "\n",
    "for class_name in dict_of_categories.keys():\n",
    "    annot_all_files = sorted(os.listdir(os.path.join(dataset_dir, dataset_name, class_name, annot_dir)))\n",
    "    for xml_file in annot_all_files:\n",
    "        annot_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Annotations/\")\n",
    "        mask_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Masks/\")\n",
    "        image_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Images/\")\n",
    "\n",
    "        tree = ET.parse(os.path.join(annot_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        delete_flags = root.findall(\"./object/deleted\")\n",
    "\n",
    "        xmins = root.findall(\"./object/segm/box/xmin\")\n",
    "        ymins = root.findall(\"./object/segm/box/ymin\")\n",
    "        xmaxs = root.findall(\"./object/segm/box/xmax\")\n",
    "        ymaxs = root.findall(\"./object/segm/box/ymax\")\n",
    "\n",
    "        mask_filenames = root.findall(\"./object/segm/mask\")\n",
    "\n",
    "        # IMAGES\n",
    "        # Image will be copied to MS COCO train directory and file name will be hold\n",
    "        image_filename = xml_retreiver(root, 'filename')\n",
    "        image = io.imread(os.path.join(image_dir, image_filename))\n",
    "        if image_id<1000000:\n",
    "            image_target_filename = \"COCO_train2014_000000\"+str(image_id)+\".jpg\"\n",
    "        elif (image_id>=1000000 or image_id<=10000000):\n",
    "            image_target_filename = \"COCO_train2014_00000\"+str(image_id)+\".jpg\"\n",
    "\n",
    "        if os.path.isfile(os.path.join(target_dataset_name, image_target_filename))==False:\n",
    "            io.imsave(os.path.join(target_dataset_name, image_target_filename), image)\n",
    "\n",
    "\n",
    "        # Image height and width\n",
    "        height = int(xml_retreiver(root, 'nrows'))\n",
    "        width = int(xml_retreiver(root, 'ncols'))\n",
    "\n",
    "        assert height == image.shape[0]\n",
    "        assert width == image.shape[1]\n",
    "\n",
    "\n",
    "        # Miscellaneous metadata\n",
    "        date_captured = DateCaptured()\n",
    "        coco_url = 'n/a'\n",
    "        flickr_url = 'n/a'\n",
    "        license = np.random.randint(8)\n",
    "\n",
    "        # Appending to 'images'\n",
    "        minival2014['images'].append({'coco_url': coco_url, 'file_name': image_target_filename, \n",
    "                                     'date_captured': date_captured, 'flickr_url': flickr_url,\n",
    "                                     'height': height, 'id': image_id, 'license': license, \n",
    "                                     'width': width})\n",
    "        # Polygon Handling\n",
    "\n",
    "        for d, delete_flag in enumerate(delete_flags):\n",
    "            delete_flag = int(delete_flag.text)\n",
    "            if not delete_flag:\n",
    "\n",
    "\n",
    "                # ANNOTATIONS\n",
    "                # Bbox\n",
    "                bbox_x = int(xmins[d].text)\n",
    "                bbox_y = int(ymins[d].text)\n",
    "                bbox_w = int(xmaxs[d].text) - bbox_x\n",
    "                bbox_h = int(ymaxs[d].text) - bbox_y\n",
    "                bbox_list = [bbox_x, bbox_y, bbox_w, bbox_h]\n",
    "\n",
    "#                 queries_findid = [d for d, image_nr in enumerate(image_nrs) if image_nr==image_filename[:-4]]\n",
    "#                 for query in queries_findid:\n",
    "#                     print(bbox_filiz[query])\n",
    "#                     print(bbox_list)\n",
    "#                 iou_for_each_xml_object = [iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid]\n",
    "#                 avg_iou.append(iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid)    \n",
    "#                 print(iou_for_each_xml_object)\n",
    "#                 print(\"\")\n",
    "\n",
    "\n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "\n",
    "#                 with open(\"bbox_info.txt\", \"a+\") as f:\n",
    "#                     f.write(image_filename[:-4]+\",\"+str(bbox_x)+\",\"+str(bbox_y)+\",\"+\n",
    "#                             str(bbox_x+bbox_w)+\",\"+str(bbox_y)+\",\"+\n",
    "#                             str(bbox_x)+\",\"+str(bbox_y+bbox_h)+\",\"+\n",
    "#                            str(bbox_x+bbox_w)+\",\"+str(bbox_y+bbox_h)+\"\\n\")\n",
    "\n",
    "                # Class\n",
    "                category_id = dict_of_categories[class_name]\n",
    "\n",
    "                # Masks Segmentation\n",
    "                instance_seg_info = []\n",
    "                mask = io.imread(os.path.join(mask_dir, mask_filenames[d].text), as_grey=True)\n",
    "                seg_coords_x, seg_coords_y = map(lambda x: list(x), np.where(mask>0))\n",
    "                seg_coords = []\n",
    "                for d, x in enumerate(seg_coords_x):\n",
    "                    seg_coords.append(x)\n",
    "                    seg_coords.append(seg_coords_y[d])\n",
    "\n",
    "\n",
    "\n",
    "                # Area\n",
    "                area = PolyArea(seg_coords_x, seg_coords_y)\n",
    "\n",
    "                # Iscrowd\n",
    "                if len(mask_filenames)>1:\n",
    "                    iscrowd = 1\n",
    "                else:\n",
    "                    iscrowd = 0\n",
    "\n",
    "                # Appending into annotations\n",
    "                minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                  'image_id': image_id, 'segmentation': [seg_coords],\n",
    "                                                  'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "                annotation_id+=1\n",
    "        image_id+=1\n",
    "\n",
    "                \n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "# print(\"Average IoU = {}\".format(sum(avg_iou)/len(avg_iou)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "# train = \"train/\"                                                # train\n",
    "# dataset_seg_dir = \"GT_segmentation/\" # one indexed version for providing segmentation data.\n",
    "\n",
    "# source_dataset_name = os.path.join(dataset_dir, \"VOT2016/\" , train)\n",
    "# source_dataset_seg_loc = os.path.join(dataset_dir, \"VOT2016/\" , dataset_seg_dir)\n",
    "# target_dataset_name = os.path.join(dataset_dir, \"coco/train2014/\")\n",
    "\n",
    "# ground_truth_bbox = \"groundtruth.txt\"\n",
    "\n",
    "# id_counter = 600000\n",
    "\n",
    "# all_video_names = [folder for folder in os.listdir(source_dataset_name) if os.path.isdir(os.path.join(source_dataset_name, folder))] \n",
    "\n",
    "# # Change: all_video_names --> dict_of_categories.keys()\n",
    "# for video_name in dict_of_categories.keys():\n",
    "#     all_frame_names = [frame for frame in os.listdir(os.path.join(source_dataset_name,\n",
    "#                                                                   video_name)) if isjpg(os.path.join(source_dataset_name, video_name, frame))]\n",
    "#     all_frame_names = sorted(all_frame_names)\n",
    "#     all_seg_frame_names = [frame for frame in os.listdir(os.path.join(source_dataset_seg_loc,\n",
    "#                                                                       video_name)) if ispng(os.path.join(source_dataset_seg_loc, video_name, frame))]\n",
    "#     all_seg_frame_names = sorted(all_seg_frame_names)\n",
    "    \n",
    "#     # bbox\n",
    "#     bbox_info_file = os.path.join(source_dataset_name, video_name, ground_truth_bbox)\n",
    "#     with open(bbox_info_file) as f:\n",
    "#         all_bboxes = f.read().split()       \n",
    "        \n",
    "#     for d, frame_name in enumerate(all_frame_names):\n",
    "            \n",
    "#         # Date captured\n",
    "#         date_captured = DateCaptured()\n",
    "        \n",
    "#         # Copying to the COCO Dataset Directory in an appropiate format.\n",
    "#         # Filename and ID will be collected from here.\n",
    "#         if id_counter<1000000:\n",
    "#             frame_file_name = \"COCO_train2014_000000\"+str(id_counter)+\".jpg\"\n",
    "#         elif (id_counter>=1000000 or id_counter<=10000000):\n",
    "#             frame_file_name = \"COCO_train2014_00000\"+str(id_counter)+\".jpg\"\n",
    "       \n",
    "#         image = io.imread(os.path.join(source_dataset_name, video_name, frame_name))\n",
    "\n",
    "#         if os.path.isfile(os.path.join(target_dataset_name, frame_file_name))==False:\n",
    "#             io.imsave(os.path.join(target_dataset_name, frame_file_name), image)\n",
    "        \n",
    "#         # License tag at random\n",
    "#         license = np.random.randint(8)\n",
    "        \n",
    "#         # Getting image width and height\n",
    "#         height = image.shape[1]\n",
    "#         width = image.shape[0]\n",
    "\n",
    "#         # URL Information is unknown\n",
    "#         url = 'n/a'\n",
    "        \n",
    "#         # Appending relevant information to images section\n",
    "# #         minival2014['images'].append({'date_captured': date_captured, 'file_name': frame_file_name, \n",
    "# #                               'height': height, 'id': id_counter, 'license': license, 'flickr_url': url, \n",
    "# #                               'width': width, 'coco_url': url})\n",
    "\n",
    "#         # bbox operations\n",
    "#         coords = np.array(all_bboxes[d].split(\",\")).reshape((-1, 2))\n",
    "#         x, y = coords[:,0], coords[:,1]\n",
    "#         xywh = coco_bbox_creator(x, y)\n",
    "        \n",
    "#         # Category ID \n",
    "#         # Create a dictionary where the keys will be video names and \n",
    "#         # values will be category id's. Then calling this dictionary\n",
    "#         # appropiately will output the category id. \n",
    "#         # E.g --> dict['fish1'] = 91\n",
    "#         category_id = 91\n",
    "        \n",
    "#         # id\n",
    "#         annot_id = 300001\n",
    "\n",
    "#         # Is crowded?\n",
    "#         iscrowd = 0\n",
    "\n",
    "#         # Image ID\n",
    "#         image_id = id_counter\n",
    "\n",
    "#         # Segmentation\n",
    "#         seg_image = io.imread(os.path.join(source_dataset_seg_loc, video_name, all_seg_frame_names[d]))/255\n",
    "#         seg_coords_x, seg_coords_y = map(lambda x: list(x), np.where(seg_image==1))\n",
    "#         seg_coords = []\n",
    "#         for d, x in enumerate(seg_coords_x):\n",
    "#             seg_coords.append(x)\n",
    "#             seg_coords.append(seg_coords_y[d])\n",
    "\n",
    "#         # Area\n",
    "#         area = PolyArea(seg_coords_x, seg_coords_y)\n",
    "        \n",
    "#         # Appending relevant info to the annotation section\n",
    "# #         minival2014['annotations'].append({'area': area, 'bbox': xywh, \n",
    "# #                                    'category_id': category_id,\n",
    "# #                                    'id': annot_id, 'image_id': image_id, 'iscrowd': iscrowd, 'segmentation': [seg_coords]})\n",
    "#         id_counter += 1\n",
    "# #         print(os.path.isfile(os.path.join(dataset_dir, source_dataset_name, train, video_name+\"/\", frame_name)))\n",
    "# #         print(dataset_dir, target_dataset_name, frame_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images in segmentation directory are zero indexed and original image directory is one indexed.\n",
    "\n",
    "# dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/VOT2016/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "# train = \"train/\"                                                # train\n",
    "# dataset_seg_dir = \"GT_segmentation/\" # one indexed version for providing segmentation data. \n",
    "\n",
    "# # Date captured\n",
    "# dt = str(datetime.datetime.now())\n",
    "# date, time = dt.split()\n",
    "# time = time.split(\".\")[0]\n",
    "# date_captured = date+\" \"+time\n",
    "\n",
    "# # Copying to the COCO Dataset Directory in an appropiate format.\n",
    "# # Filename and ID will be collected from here.\n",
    "# target_dataset_dir = \"Datasets/coco/train2014/\"\n",
    "# idx = 600001\n",
    "# file_name = \"COCO_train2014_000000\"+str(idx)+\".jpg\"\n",
    "# image = io.imread(image_dir)\n",
    "# io.imsave(target_dataset_dir+file_name, image)\n",
    "\n",
    "# # License tag at random\n",
    "# license = np.random.randint(8)\n",
    "\n",
    "# # Getting image width and height\n",
    "# height = image.shape[1]\n",
    "# width = image.shape[0]\n",
    "\n",
    "# # URL Information is unknown\n",
    "# url = 'n/a'\n",
    "# # minival2014['images'].append({'date_captured': date_captured, 'file_name': file_name, \n",
    "# #                               'height': height, 'id': idx, 'license': license, 'flickr_url': url, \n",
    "# #                               'width': width, 'coco_url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (minival2014['images'][-3:])\n",
    "# minival2014['images'].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations\n",
    "\n",
    "# def coco_bbox_creator(x, y):\n",
    "#     x = list(map(lambda x: float(x), x))\n",
    "#     y = list(map(lambda x: float(x), y))\n",
    "#     x_min = min(x)\n",
    "#     y_min = min(y)\n",
    "#     w = max(x) - x_min\n",
    "#     h = max(y) - y_min\n",
    "#     return [x_min, y_min, w, h]\n",
    "\n",
    "# # bbox\n",
    "# bbox_info_file = \"/home/mspr/Desktop/Mask_RCNN/Datasets/VOT2016/train/fish1/groundtruth.txt\"\n",
    "# with open(bbox_info_file) as f:\n",
    "#     all_bboxes = f.read().split()\n",
    "#     for bbox in all_bboxes[1:2]:\n",
    "#         coords = np.array(bbox.split(\",\")).reshape((-1, 2)) \n",
    "#         x, y = coords[:,0], coords[:,1]\n",
    "#         xywh = coco_bbox_creator(x, y)\n",
    "#         print(xywh)\n",
    "\n",
    "# # category id\n",
    "# # Create a dictionary where the keys will be video names and \n",
    "# # values will be category id's. Then calling this dictionary\n",
    "# # appropiately will output the category id. \n",
    "# # E.g --> dict['fish1'] = 91\n",
    "# category_id = 91\n",
    "\n",
    "# # id\n",
    "# annot_id = 300001\n",
    "    \n",
    "# # Is crowded?\n",
    "# iscrowd = 0\n",
    "\n",
    "# # Image ID\n",
    "# image_id = idx\n",
    "\n",
    "# # Segmentation\n",
    "# seg_image = io.imread(seg_image_dir)/255\n",
    "# seg_coords_x, seg_coords_y = map(lambda x: list(x), np.where(seg_image==1))\n",
    "# seg_coords = []\n",
    "# for d, x in enumerate(seg_coords_x):\n",
    "#     seg_coords.append(x)\n",
    "#     seg_coords.append(seg_coords_y[d])\n",
    "\n",
    "# # Area\n",
    "# def PolyArea(x,y):\n",
    "#     return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "# area = PolyArea(seg_coords_x, seg_coords_y)\n",
    "\n",
    "# minival2014['annotations'].append({'area': area, 'bbox': xywh, \n",
    "#                                    'category_id': category_id,\n",
    "#                                    'id': annot_id, 'image_id': idx, 'iscrowd': iscrowd, 'segmentation': [seg_coords]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minival2014['annotations'].pop()\n",
    "# print (minival2014['annotations'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in memory and then writes them into a file. \n",
    "\n",
    "# from decimal import Decimal\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "with open(\"Datasets/coco/annotations/instances_train2014.json\",\"w\") as f:\n",
    "# with open(\"Datasets/coco/annotations/instances_valminusminival2014.json\",\"w\") as f:\n",
    "    data = json.dumps(minival2014, cls=MyEncoder)\n",
    "    f.write(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
