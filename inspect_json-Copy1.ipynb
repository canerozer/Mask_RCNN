{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import datetime\n",
    "import os\n",
    "from pycocotools import coco\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CATEGORY INFORMATION\n",
    "### !!! NEED HELP FOR MATCHING THE CLASSES IN VOT2016 TO THEIR CORRESPONDING ONES IN MS-COCO !!! ### \n",
    "### Legend ###\n",
    "### * --> Will be defined into Mask R-CNN as a new category\n",
    "### ! --> Might be some issues with the assignment, they have to be rechecked\n",
    "dict_of_categories = {'face': 91}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'images', 'licenses', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dir = \"Datasets/coco/annotations_old/instances_train2014.json\"\n",
    "\n",
    "minival2014 = json.load(open(json_dir))\n",
    "\n",
    "minival2014.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = minival2014['annotations'][604864]\n",
    "# print(list['segmentation'])\n",
    "# for d, i in enumerate(list):\n",
    "#     if i['iscrowd']:\n",
    "#         print(d)\n",
    "#         print(i['segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the classes, view the category list at first!!\n",
    "# minival2014['categories'].append({'id': 91, 'name': 'face', 'supercategory': 'person'})\n",
    "# minival2014['categories'].append({'id': 92, 'name': 'fish', 'supercategory': 'animal'})\n",
    "# minival2014['categories']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the added class at the bottom.\n",
    "# minival2014['images'].pop()\n",
    "# (minival2014['images'][-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to be used:\n",
    "def isjpg(string):\n",
    "    if string[-4:]==\".jpg\":\n",
    "        return True \n",
    "    \n",
    "def ispng(string):\n",
    "    if string[-4:]==\".png\":\n",
    "        return True \n",
    "    \n",
    "def DateCaptured():\n",
    "    dt = str(datetime.datetime.now())\n",
    "    date, time = dt.split()\n",
    "    time = time.split(\".\")[0]\n",
    "    return date+\" \"+time\n",
    "\n",
    "def coco_bbox_creator(x, y):\n",
    "    x = list(map(lambda x: float(x), x))\n",
    "    y = list(map(lambda x: float(x), y))\n",
    "    x_min = min(x)\n",
    "    y_min = min(y)\n",
    "    w = max(x) - x_min\n",
    "    h = max(y) - y_min\n",
    "    return [x_min, y_min, w, h]\n",
    "\n",
    "def PolyArea(x,y):\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def xml_retreiver(tree_object, key_name):\n",
    "    for iterator in tree_object.iter(key_name):\n",
    "        return iterator.text\n",
    "    \n",
    "import pycocotools._mask as _mask\n",
    "encode_mask = _mask.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelMe type\n",
    "dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "dataset_name = \"MSPR_Dataset/\"\n",
    "annot_dir = \"Annotations/\"\n",
    "mask_dir = \"Masks/\"\n",
    "image_dir = \"Images/\"\n",
    "\n",
    "target_dataset_name = os.path.join(dataset_dir, \"coco/train2014/\")\n",
    "\n",
    "# Added image and annotation ids will start from 600k. \n",
    "image_id = 600000\n",
    "annotation_id = 600000\n",
    "\n",
    "for class_name in dict_of_categories.keys():\n",
    "    annot_all_files = sorted(os.listdir(os.path.join(dataset_dir, dataset_name, class_name, annot_dir)))\n",
    "    for xml_file in annot_all_files:\n",
    "        annot_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Annotations/\")\n",
    "        mask_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Masks/\")\n",
    "        image_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Images/\")\n",
    "\n",
    "        tree = ET.parse(os.path.join(annot_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        delete_flags = root.findall(\"./object/deleted\")\n",
    "#         bbox_x = float(xml_retreiver(root, 'xmin'))\n",
    "#         bbox_y = float(xml_retreiver(root, 'ymin'))\n",
    "#         bbox_w = float(xml_retreiver(root, 'xmax')) - bbox_x\n",
    "#         bbox_h = float(xml_retreiver(root, 'ymax')) - bbox_y\n",
    "        \n",
    "        xmins = root.findall(\"./object/segm/box/xmin\")\n",
    "        ymins = root.findall(\"./object/segm/box/ymin\")\n",
    "        xmaxs = root.findall(\"./object/segm/box/xmax\")\n",
    "        ymaxs = root.findall(\"./object/segm/box/ymax\")\n",
    "        \n",
    "        mask_filenames = root.findall(\"./object/segm/mask\")\n",
    "\n",
    "        # Polygon Handling\n",
    "        \n",
    "        for d, delete_flag in enumerate(delete_flags):\n",
    "            delete_flag = int(delete_flag.text)\n",
    "            if not delete_flag:\n",
    "                # IMAGES\n",
    "                # Image will be copied to MS COCO train directory and file name will be hold\n",
    "                image_filename = xml_retreiver(root, 'filename')\n",
    "                image = io.imread(os.path.join(image_dir, image_filename))\n",
    "                if image_id<1000000:\n",
    "                    image_target_filename = \"COCO_train2014_000000\"+str(image_id)+\".jpg\"\n",
    "                elif (image_id>=1000000 or image_id<=10000000):\n",
    "                    image_target_filename = \"COCO_train2014_00000\"+str(image_id)+\".jpg\"\n",
    "\n",
    "                if os.path.isfile(os.path.join(target_dataset_name, image_target_filename))==False:\n",
    "                    io.imsave(os.path.join(target_dataset_name, image_target_filename), image)\n",
    "\n",
    "\n",
    "                # Image height and width\n",
    "                height = int(xml_retreiver(root, 'nrows'))\n",
    "                width = int(xml_retreiver(root, 'ncols'))\n",
    "\n",
    "                assert height == image.shape[0]\n",
    "                assert width == image.shape[1]\n",
    "\n",
    "\n",
    "                # Miscellaneous metadata\n",
    "                date_captured = 'n/a'\n",
    "                url = 'n/a'\n",
    "                license = np.random.randn(8)\n",
    "\n",
    "                # ANNOTATIONS\n",
    "                # Bbox\n",
    "                bbox_x = int(xmins[d].text)\n",
    "                bbox_y = int(ymins[d].text)\n",
    "                bbox_w = int(xmaxs[d].text) - bbox_x\n",
    "                bbox_h = int(ymaxs[d].text) - bbox_y\n",
    "                \n",
    "                with open(\"bbox_info.txt\", \"a+\") as f:\n",
    "                    f.write(image_filename[:-4]+\",\"+str(bbox_x)+\",\"+str(bbox_y)+\",\"+\n",
    "                            str(bbox_x+bbox_w)+\",\"+str(bbox_y)+\",\"+\n",
    "                            str(bbox_x)+\",\"+str(bbox_y+bbox_h)+\",\"+\n",
    "                           str(bbox_x+bbox_w)+\",\"+str(bbox_y+bbox_h)+\"\\n\")\n",
    "\n",
    "                # Class\n",
    "                category_id = dict_of_categories[class_name]\n",
    "\n",
    "                # Masks\n",
    "                instance_seg_info = []\n",
    "                mask = io.imread(os.path.join(mask_dir, mask_filenames[d].text))[:,:,:4] \n",
    "                mask = np.asfortranarray(mask)\n",
    "                instance_seg_info.append(_mask.encode(mask))   \n",
    "\n",
    "                # Iscrowd\n",
    "                if len(mask_filenames)>1:\n",
    "                    iscrowd = 1\n",
    "                else:\n",
    "                    iscrowd = 0\n",
    "\n",
    "\n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "train = \"train/\"                                                # train\n",
    "dataset_seg_dir = \"GT_segmentation/\" # one indexed version for providing segmentation data.\n",
    "\n",
    "source_dataset_name = os.path.join(dataset_dir, \"VOT2016/\" , train)\n",
    "source_dataset_seg_loc = os.path.join(dataset_dir, \"VOT2016/\" , dataset_seg_dir)\n",
    "target_dataset_name = os.path.join(dataset_dir, \"coco/train2014/\")\n",
    "\n",
    "ground_truth_bbox = \"groundtruth.txt\"\n",
    "\n",
    "id_counter = 600000\n",
    "\n",
    "all_video_names = [folder for folder in os.listdir(source_dataset_name) if os.path.isdir(os.path.join(source_dataset_name, folder))] \n",
    "\n",
    "# Change: all_video_names --> dict_of_categories.keys()\n",
    "for video_name in dict_of_categories.keys():\n",
    "    all_frame_names = [frame for frame in os.listdir(os.path.join(source_dataset_name,\n",
    "                                                                  video_name)) if isjpg(os.path.join(source_dataset_name, video_name, frame))]\n",
    "    all_frame_names = sorted(all_frame_names)\n",
    "    all_seg_frame_names = [frame for frame in os.listdir(os.path.join(source_dataset_seg_loc,\n",
    "                                                                      video_name)) if ispng(os.path.join(source_dataset_seg_loc, video_name, frame))]\n",
    "    all_seg_frame_names = sorted(all_seg_frame_names)\n",
    "    \n",
    "    # bbox\n",
    "    bbox_info_file = os.path.join(source_dataset_name, video_name, ground_truth_bbox)\n",
    "    with open(bbox_info_file) as f:\n",
    "        all_bboxes = f.read().split()       \n",
    "        \n",
    "    for d, frame_name in enumerate(all_frame_names):\n",
    "            \n",
    "        # Date captured\n",
    "        date_captured = DateCaptured()\n",
    "        \n",
    "        # Copying to the COCO Dataset Directory in an appropiate format.\n",
    "        # Filename and ID will be collected from here.\n",
    "        if id_counter<1000000:\n",
    "            frame_file_name = \"COCO_train2014_000000\"+str(id_counter)+\".jpg\"\n",
    "        elif (id_counter>=1000000 or id_counter<=10000000):\n",
    "            frame_file_name = \"COCO_train2014_00000\"+str(id_counter)+\".jpg\"\n",
    "       \n",
    "        image = io.imread(os.path.join(source_dataset_name, video_name, frame_name))\n",
    "\n",
    "        if os.path.isfile(os.path.join(target_dataset_name, frame_file_name))==False:\n",
    "            io.imsave(os.path.join(target_dataset_name, frame_file_name), image)\n",
    "        \n",
    "        # License tag at random\n",
    "        license = np.random.randint(8)\n",
    "        \n",
    "        # Getting image width and height\n",
    "        height = image.shape[1]\n",
    "        width = image.shape[0]\n",
    "\n",
    "        # URL Information is unknown\n",
    "        url = 'n/a'\n",
    "        \n",
    "        # Appending relevant information to images section\n",
    "#         minival2014['images'].append({'date_captured': date_captured, 'file_name': frame_file_name, \n",
    "#                               'height': height, 'id': id_counter, 'license': license, 'flickr_url': url, \n",
    "#                               'width': width, 'coco_url': url})\n",
    "\n",
    "        # bbox operations\n",
    "        coords = np.array(all_bboxes[d].split(\",\")).reshape((-1, 2))\n",
    "        x, y = coords[:,0], coords[:,1]\n",
    "        xywh = coco_bbox_creator(x, y)\n",
    "        \n",
    "        # Category ID \n",
    "        # Create a dictionary where the keys will be video names and \n",
    "        # values will be category id's. Then calling this dictionary\n",
    "        # appropiately will output the category id. \n",
    "        # E.g --> dict['fish1'] = 91\n",
    "        category_id = 91\n",
    "        \n",
    "        # id\n",
    "        annot_id = 300001\n",
    "\n",
    "        # Is crowded?\n",
    "        iscrowd = 0\n",
    "\n",
    "        # Image ID\n",
    "        image_id = id_counter\n",
    "\n",
    "        # Segmentation\n",
    "        seg_image = io.imread(os.path.join(source_dataset_seg_loc, video_name, all_seg_frame_names[d]))/255\n",
    "        seg_coords_x, seg_coords_y = map(lambda x: list(x), np.where(seg_image==1))\n",
    "        seg_coords = []\n",
    "        for d, x in enumerate(seg_coords_x):\n",
    "            seg_coords.append(x)\n",
    "            seg_coords.append(seg_coords_y[d])\n",
    "\n",
    "        # Area\n",
    "        area = PolyArea(seg_coords_x, seg_coords_y)\n",
    "        \n",
    "        # Appending relevant info to the annotation section\n",
    "#         minival2014['annotations'].append({'area': area, 'bbox': xywh, \n",
    "#                                    'category_id': category_id,\n",
    "#                                    'id': annot_id, 'image_id': image_id, 'iscrowd': iscrowd, 'segmentation': [seg_coords]})\n",
    "        id_counter += 1\n",
    "#         print(os.path.isfile(os.path.join(dataset_dir, source_dataset_name, train, video_name+\"/\", frame_name)))\n",
    "#         print(dataset_dir, target_dataset_name, frame_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images in segmentation directory are zero indexed and original image directory is one indexed.\n",
    "\n",
    "# dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/VOT2016/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "# train = \"train/\"                                                # train\n",
    "# dataset_seg_dir = \"GT_segmentation/\" # one indexed version for providing segmentation data. \n",
    "\n",
    "# # Date captured\n",
    "# dt = str(datetime.datetime.now())\n",
    "# date, time = dt.split()\n",
    "# time = time.split(\".\")[0]\n",
    "# date_captured = date+\" \"+time\n",
    "\n",
    "# # Copying to the COCO Dataset Directory in an appropiate format.\n",
    "# # Filename and ID will be collected from here.\n",
    "# target_dataset_dir = \"Datasets/coco/train2014/\"\n",
    "# idx = 600001\n",
    "# file_name = \"COCO_train2014_000000\"+str(idx)+\".jpg\"\n",
    "# image = io.imread(image_dir)\n",
    "# io.imsave(target_dataset_dir+file_name, image)\n",
    "\n",
    "# # License tag at random\n",
    "# license = np.random.randint(8)\n",
    "\n",
    "# # Getting image width and height\n",
    "# height = image.shape[1]\n",
    "# width = image.shape[0]\n",
    "\n",
    "# # URL Information is unknown\n",
    "# url = 'n/a'\n",
    "# # minival2014['images'].append({'date_captured': date_captured, 'file_name': file_name, \n",
    "# #                               'height': height, 'id': idx, 'license': license, 'flickr_url': url, \n",
    "# #                               'width': width, 'coco_url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (minival2014['images'][-3:])\n",
    "# minival2014['images'].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations\n",
    "\n",
    "# def coco_bbox_creator(x, y):\n",
    "#     x = list(map(lambda x: float(x), x))\n",
    "#     y = list(map(lambda x: float(x), y))\n",
    "#     x_min = min(x)\n",
    "#     y_min = min(y)\n",
    "#     w = max(x) - x_min\n",
    "#     h = max(y) - y_min\n",
    "#     return [x_min, y_min, w, h]\n",
    "\n",
    "# # bbox\n",
    "# bbox_info_file = \"/home/mspr/Desktop/Mask_RCNN/Datasets/VOT2016/train/fish1/groundtruth.txt\"\n",
    "# with open(bbox_info_file) as f:\n",
    "#     all_bboxes = f.read().split()\n",
    "#     for bbox in all_bboxes[1:2]:\n",
    "#         coords = np.array(bbox.split(\",\")).reshape((-1, 2)) \n",
    "#         x, y = coords[:,0], coords[:,1]\n",
    "#         xywh = coco_bbox_creator(x, y)\n",
    "#         print(xywh)\n",
    "\n",
    "# # category id\n",
    "# # Create a dictionary where the keys will be video names and \n",
    "# # values will be category id's. Then calling this dictionary\n",
    "# # appropiately will output the category id. \n",
    "# # E.g --> dict['fish1'] = 91\n",
    "# category_id = 91\n",
    "\n",
    "# # id\n",
    "# annot_id = 300001\n",
    "    \n",
    "# # Is crowded?\n",
    "# iscrowd = 0\n",
    "\n",
    "# # Image ID\n",
    "# image_id = idx\n",
    "\n",
    "# # Segmentation\n",
    "# seg_image = io.imread(seg_image_dir)/255\n",
    "# seg_coords_x, seg_coords_y = map(lambda x: list(x), np.where(seg_image==1))\n",
    "# seg_coords = []\n",
    "# for d, x in enumerate(seg_coords_x):\n",
    "#     seg_coords.append(x)\n",
    "#     seg_coords.append(seg_coords_y[d])\n",
    "\n",
    "# # Area\n",
    "# def PolyArea(x,y):\n",
    "#     return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "# area = PolyArea(seg_coords_x, seg_coords_y)\n",
    "\n",
    "# minival2014['annotations'].append({'area': area, 'bbox': xywh, \n",
    "#                                    'category_id': category_id,\n",
    "#                                    'id': annot_id, 'image_id': idx, 'iscrowd': iscrowd, 'segmentation': [seg_coords]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minival2014['annotations'].pop()\n",
    "# print (minival2014['annotations'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in memory and then writes them into a file. \n",
    "\n",
    "# from decimal import Decimal\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "with open(\"Datasets/coco/annotations/instances_minival2014.json\",\"w\") as f:\n",
    "    data = json.dumps(minival2014, cls=MyEncoder)\n",
    "    f.write(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
