{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import os\n",
    "from pycocotools import coco\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml\n",
    "from pycocotools import mask as comask\n",
    "import cv2\n",
    "from utils import DateCaptured, coco_bbox_creator, xml_retreiver, iou_calculator, biggest_segment, mask2poly, binary_mask_to_rle\n",
    "# import csv ozan changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CATEGORY INFORMATION\n",
    "\n",
    "# Add the name of the folders inside the dataset and\n",
    "# assign some label id. Start iding from 91.\n",
    "\n",
    "dict_of_categories = {'face': 91}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET INFORMATION\n",
    "\n",
    "# Enter the original/custom JSON file to be used.\n",
    "# I used the original train2014 JSON file containing\n",
    "# the instances.\n",
    "\n",
    "json_dir = \"Datasets/coco/annotations_old/instances_train2014.json\"\n",
    "\n",
    "# for the purpose of creating validation images json\n",
    "# json_dir = \"Datasets/coco/annotations_old/instances_val2014.json\" \n",
    "\n",
    "# for the purpose of creating testing images json\n",
    "# json_dir = \"Datasets/coco/annotations_old/instances_minival2014.json\" \n",
    "\n",
    "### For viewing the content of the new json file. ###\n",
    "# json_dir = \"Datasets/coco/annotations/instances_train2014.json\"\n",
    "\n",
    "### minival2014 will become the main dictionary ###\n",
    "minival2014 = json.load(open(json_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEARING THE DATASET\n",
    "# Use that if you only going to make a dataset of your own.\n",
    "# If you are going to append the images, this operation is not necessary.\n",
    "\n",
    "minival2014['images'] = []\n",
    "minival2014['annotations'] = []\n",
    "# minival2014['categories'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the number of instances per class and gathering some statistics about them.\n",
    "# labels = np.zeros((91, ), dtype=np.int32)\n",
    "# for annotation in minival2014['annotations']:\n",
    "#     labels[annotation['category_id']] += 1\n",
    "# used_label_ids = np.where(labels!=0)\n",
    "# ex_labels = labels[used_label_ids]\n",
    "# print(ex_labels)\n",
    "# print(\"Average instances per label: {}\".format(sum(ex_labels)/81))\n",
    "# print(\"Maximum instances per label: {}\".format(max(ex_labels)))\n",
    "# print(\"Minimum instances per label: {}\".format(min(ex_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPENDING CLASSES TO THE CATEGORIES\n",
    "\n",
    "### For viewing the categories inside the JSON file.\n",
    "# minival2014['categories']\n",
    "\n",
    "# Add the classes based on to the dict_of_categories\n",
    "# Below shows an example for adding the face class to the main dictionary. \n",
    "\n",
    "minival2014['categories'].append({'id': 91, 'name': 'face', 'supercategory': 'person'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to be used:\n",
    "def isjpg(string):\n",
    "    if string[-4:]==\".jpg\":\n",
    "        return True \n",
    "    \n",
    "def ispng(string):\n",
    "    if string[-4:]==\".png\":\n",
    "        return True\n",
    "    \n",
    "def save_bboxes(image_name, bbox_coor):\n",
    "    with open('img_bbox.txt','a+') as file:\n",
    "        file.write(str(image_name)+','+str(bbox_coor[0])+','+str(bbox_coor[1])+','+str(bbox_coor[2])+','+str(bbox_coor[3])+'\\n')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000001.xml 000021.xml 000025.xml "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/morphology/misc.py:122: UserWarning: Only one label was provided to `remove_small_objects`. Did you mean to use a boolean array?\n",
      "  warn(\"Only one label was provided to `remove_small_objects`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000027.xml 000030.xml 000035.xml 000038.xml 000043.xml 000050.xml 000051.xml 000058.xml 000069.xml 000073.xml 000076.xml 000081.xml 000085.xml 000089.xml 000090.xml 000096.xml 000101.xml 000104.xml 000105.xml 000113.xml 000124.xml 000125.xml 000127.xml 000129.xml 000133.xml 000138.xml 000139.xml 000144.xml 000146.xml 000151.xml 000159.xml 000162.xml 000164.xml 000165.xml 000166.xml 000169.xml 000170.xml 000171.xml 000173.xml 000174.xml 000177.xml 000181.xml 000182.xml 000191.xml 000192.xml 000193.xml 000200.xml 000202.xml 000205.xml 000206.xml 000212.xml 000220.xml 000222.xml 000229.xml 000230.xml 000231.xml 000237.xml 000245.xml 000247.xml 000248.xml 000252.xml 000257.xml 000258.xml 000259.xml 000264.xml 000265.xml 000269.xml 000271.xml 000272.xml 000275.xml 000276.xml 000278.xml 000280.xml 000282.xml 000283.xml 000285.xml 000286.xml 000287.xml 000297.xml 000298.xml 000299.xml 000302.xml 000305.xml 000308.xml 000310.xml 000315.xml 000319.xml 000320.xml 000321.xml 000322.xml 000323.xml 000328.xml 000331.xml 000337.xml 000339.xml 000342.xml 000346.xml 000348.xml 000352.xml 000356.xml 000359.xml 000367.xml 000368.xml 000369.xml 000372.xml 000374.xml 000377.xml 000378.xml 000386.xml 000388.xml 000392.xml 000393.xml 000394.xml 000405.xml 000407.xml 000409.xml 000413.xml 000414.xml 000423.xml 000428.xml 000433.xml 000434.xml 000438.xml 000446.xml 000448.xml 000449.xml 000453.xml 000457.xml 000468.xml 000476.xml 000477.xml 000479.xml 000483.xml 000485.xml 000490.xml 000493.xml 000497.xml 000498.xml 000499.xml 000500.xml 000502.xml 000506.xml 000507.xml 000516.xml 000517.xml 000520.xml 000523.xml 000524.xml 000526.xml 000530.xml 000531.xml 000534.xml 000535.xml 000539.xml 000545.xml 000546.xml 000555.xml 000562.xml 000566.xml 000567.xml 000578.xml 000586.xml 000587.xml 000589.xml 000594.xml 000602.xml 000604.xml 000606.xml 000612.xml 000613.xml 000615.xml 000616.xml 000617.xml 000624.xml 000633.xml 000639.xml 000641.xml 000642.xml 000643.xml 000644.xml 000652.xml 000662.xml 000664.xml 000670.xml 000677.xml 000683.xml 000828.xml 000839.xml 000843.xml 000847.xml 000848.xml 000851.xml 000854.xml 000856.xml 000859.xml 000860.xml 000861.xml 000865.xml 000870.xml 000874.xml 000878.xml 000879.xml 000886.xml 000892.xml 000895.xml 000903.xml 000909.xml 000910.xml 000915.xml 000916.xml 000918.xml 000922.xml 000926.xml 000940.xml 000943.xml 000952.xml 000955.xml 000956.xml 000959.xml 000969.xml 000978.xml 000981.xml 000984.xml 000987.xml 000988.xml 000999.xml 001014.xml 001020.xml 001021.xml 001024.xml 001026.xml 001028.xml 001033.xml 001036.xml 001037.xml 001038.xml 001040.xml 001047.xml 001055.xml 001061.xml 001065.xml 001067.xml 001071.xml 001072.xml 001079.xml 001086.xml 001091.xml 001095.xml 001097.xml 001105.xml 001108.xml 001109.xml 001113.xml 001116.xml 001118.xml 001129.xml 001133.xml 001137.xml 001140.xml 001147.xml 001150.xml 001164.xml 001167.xml 001170.xml 001173.xml 001185.xml 001220.xml 001228.xml 001229.xml 001234.xml 001236.xml 001242.xml 001243.xml 001244.xml 001251.xml 001253.xml 001261.xml 001265.xml 001272.xml 001279.xml 001282.xml 001284.xml 001287.xml 001297.xml 001307.xml 001309.xml 001310.xml 001311.xml 001315.xml 001319.xml 001320.xml 001325.xml 001327.xml 001330.xml 001333.xml 001336.xml 001337.xml 001340.xml 001346.xml 001347.xml 001350.xml 001351.xml 001353.xml 001354.xml 001358.xml 001362.xml 001366.xml 001368.xml 001370.xml 001376.xml 001388.xml 001390.xml 001393.xml 001406.xml 001408.xml 001409.xml 001411.xml 001417.xml 001419.xml 001493.xml 001495.xml 001496.xml 001498.xml 001502.xml 001514.xml 001516.xml 001521.xml 001523.xml 001526.xml 001531.xml 001533.xml 001537.xml 001544.xml 001548.xml 001554.xml 001558.xml 001561.xml 001563.xml 001564.xml 001566.xml 001569.xml 001570.xml 001571.xml 001579.xml 001580.xml 001581.xml 001583.xml 001585.xml 001593.xml 001606.xml 001608.xml 001611.xml 001620.xml 001627.xml 001629.xml 001630.xml 001644.xml 001649.xml 001651.xml 001652.xml 001657.xml 001682.xml 001686.xml 001689.xml 001710.xml 001712.xml 001730.xml 001735.xml 001743.xml 001749.xml 001754.xml 001756.xml 001757.xml 001759.xml 001760.xml 001769.xml 001776.xml 001781.xml 001784.xml 001785.xml 001791.xml 001792.xml 001793.xml 001794.xml 001797.xml 001800.xml 001802.xml 001805.xml 001808.xml 001810.xml 001812.xml 001828.xml 001830.xml 001831.xml 001833.xml 001836.xml 001837.xml 001841.xml 001843.xml 001849.xml 001854.xml 001857.xml 001861.xml 001864.xml 001875.xml 001886.xml 001889.xml 001891.xml 001893.xml 001894.xml 001904.xml 001914.xml 001918.xml 001920.xml 001921.xml 001932.xml 001938.xml 001943.xml 001946.xml 001947.xml 001958.xml 001963.xml 001968.xml 001975.xml 001977.xml 001978.xml 001980.xml 001986.xml 001988.xml 001989.xml 001993.xml 001998.xml 001999.xml 002008.xml 002010.xml 002012.xml 002018.xml 002024.xml 002026.xml 002030.xml 002031.xml 002041.xml 002051.xml 002055.xml 002060.xml 002061.xml 002065.xml 002069.xml 002071.xml 002072.xml 002074.xml 002077.xml 002079.xml 002082.xml 002087.xml 002090.xml 002096.xml 002097.xml 002104.xml 002108.xml 002111.xml 002114.xml 002117.xml 002119.xml 002120.xml 002121.xml 002137.xml 002142.xml 002145.xml 002159.xml 002163.xml 002164.xml 002169.xml 002172.xml 002176.xml 002183.xml 002187.xml 002189.xml 002190.xml 002193.xml 002194.xml 002195.xml 002203.xml 002207.xml 002211.xml 002213.xml 002219.xml 002220.xml 002221.xml 002224.xml 002226.xml 002230.xml 002233.xml 002235.xml 002238.xml 002239.xml 002241.xml 002242.xml 002250.xml 002254.xml 002262.xml 002272.xml 002273.xml 002285.xml 002293.xml 002295.xml 002298.xml 002301.xml 002302.xml 002306.xml 002307.xml 002314.xml 002317.xml 002324.xml 002325.xml 002329.xml 002331.xml 002332.xml 002337.xml 002341.xml 002342.xml 002344.xml 002365.xml 002378.xml 002382.xml 002384.xml 002385.xml 002399.xml 002405.xml 002410.xml 002413.xml 002421.xml 002426.xml 002428.xml 002429.xml 002433.xml 002434.xml 002438.xml 002443.xml 002444.xml 002446.xml 002455.xml 002460.xml 002463.xml 002483.xml 002488.xml 002491.xml 002498.xml 002501.xml 002503.xml 002509.xml 002511.xml 002514.xml 002516.xml 002518.xml 002524.xml 002528.xml 002538.xml 002540.xml 002542.xml 002545.xml 002550.xml 002551.xml 002554.xml 002562.xml 002564.xml 002570.xml 002572.xml 002575.xml 002584.xml 002588.xml 002589.xml 002591.xml 002595.xml 002596.xml 002604.xml 002613.xml 002614.xml 002620.xml 002624.xml 002626.xml 002632.xml 002633.xml 002637.xml 002641.xml 002649.xml 002651.xml 002652.xml 002655.xml 002656.xml 002660.xml 002662.xml 002668.xml 002671.xml 002673.xml 002675.xml 002678.xml 002686.xml 002687.xml 002689.xml 002691.xml 002692.xml 002693.xml 002700.xml 002705.xml 002710.xml 002712.xml 002713.xml 002714.xml 002726.xml 002727.xml 002741.xml 002744.xml 002750.xml 002755.xml 002761.xml 002773.xml 002781.xml 002788.xml 002792.xml 002798.xml 002815.xml 002823.xml 002824.xml 005218.xml 005220.xml 005225.xml 005226.xml 005227.xml 005229.xml 005231.xml 005236.xml 005239.xml 005242.xml 005245.xml 005251.xml 005252.xml 005261.xml 005274.xml 005283.xml 005286.xml 005291.xml 005301.xml 005304.xml 005306.xml 005308.xml 005310.xml 005311.xml 005322.xml 005327.xml 005343.xml 005346.xml 005348.xml 005356.xml 005357.xml 005368.xml 005371.xml 005374.xml 005377.xml 005383.xml 005384.xml 005385.xml 005386.xml 005392.xml 005396.xml 005399.xml 005409.xml 005411.xml 005413.xml 005414.xml 005417.xml 005421.xml 005425.xml 005430.xml 005439.xml 005441.xml 005442.xml 005447.xml 005449.xml 005450.xml 005459.xml 005461.xml 005467.xml 005468.xml 005475.xml 005477.xml 005478.xml 005482.xml 005488.xml 005491.xml 005494.xml 005500.xml 005508.xml 005527.xml 005533.xml 005536.xml 005541.xml 005542.xml 005546.xml 005548.xml 005553.xml 005556.xml 005563.xml 005570.xml 008285.xml 008287.xml 008309.xml 008312.xml 008321.xml 008325.xml 008332.xml 008341.xml 008363.xml 008367.xml 008368.xml 008374.xml 008377.xml 008381.xml 008385.xml 008390.xml 008392.xml 008398.xml 008400.xml 008402.xml 008407.xml 008410.xml 008411.xml 008413.xml 008418.xml 008430.xml 008436.xml 008439.xml 008446.xml 008451.xml 008453.xml 008458.xml 008487.xml 008505.xml 008510.xml 008513.xml 008520.xml 008528.xml 008529.xml 008538.xml 008559.xml 008566.xml 008567.xml 008569.xml 008573.xml 008576.xml 008577.xml 008583.xml 008585.xml 008587.xml 008592.xml 008602.xml 008617.xml 008629.xml 008641.xml 008664.xml 008666.xml 008678.xml 008681.xml 008682.xml 008698.xml 008699.xml 008700.xml 008701.xml 008708.xml 008721.xml 008726.xml 008728.xml 008740.xml 008758.xml 008762.xml 008769.xml 008774.xml 008808.xml 008816.xml 008817.xml 008833.xml 008836.xml 008857.xml 008862.xml 008864.xml 008868.xml 008872.xml 008879.xml 008884.xml 008894.xml 008897.xml 008902.xml 008916.xml 008918.xml 008921.xml 008928.xml 008931.xml 008932.xml 008941.xml 008944.xml 008946.xml 008957.xml 008961.xml 008963.xml 008965.xml 008974.xml 008993.xml 009017.xml 009040.xml 009043.xml 009046.xml 009064.xml 009075.xml 009079.xml 009087.xml 009115.xml 009125.xml 009127.xml 009128.xml 009136.xml 009141.xml 009152.xml 009153.xml 009164.xml 009171.xml 009180.xml 009184.xml 009193.xml 009208.xml 009238.xml 009239.xml 009242.xml 009248.xml 009253.xml 009273.xml 009287.xml 009291.xml 009294.xml 009295.xml 009297.xml 009299.xml 009306.xml 009312.xml 009314.xml 009316.xml 009330.xml 009342.xml 009343.xml 009349.xml 009362.xml 009363.xml 009364.xml 009370.xml 009375.xml 009382.xml 009391.xml 009393.xml 009402.xml 009410.xml 009412.xml 009414.xml 009415.xml 009418.xml 009420.xml 009421.xml 009424.xml 009425.xml 009428.xml 009437.xml 009441.xml 009478.xml 009479.xml 009490.xml 009496.xml 009497.xml 009499.xml 009500.xml 009517.xml 009520.xml 009525.xml 009529.xml 009535.xml 009546.xml 009547.xml 009551.xml 009556.xml 009561.xml 009567.xml 009574.xml 009577.xml 009578.xml 009579.xml 009585.xml 009599.xml 009603.xml 009610.xml 009612.xml 009617.xml 009622.xml 009628.xml 009633.xml 009638.xml 009647.xml 009648.xml 009649.xml 009651.xml 009653.xml 009654.xml 009655.xml 009666.xml 009668.xml 009684.xml 009687.xml 009693.xml 009696.xml 009698.xml 009708.xml 009712.xml 009715.xml 009721.xml 009726.xml 009736.xml 009739.xml 009741.xml 009755.xml 009766.xml 009767.xml 009773.xml 009776.xml 009779.xml 009780.xml 009782.xml 009783.xml 009789.xml 009792.xml 009796.xml 009798.xml 009800.xml 009802.xml 009809.xml 009812.xml 009813.xml 009828.xml 009848.xml 009853.xml 009854.xml 009857.xml 009859.xml 009868.xml 009869.xml 009871.xml 009878.xml 009880.xml 009881.xml 009882.xml 009885.xml 009892.xml 009901.xml 009903.xml 009915.xml 009917.xml 009918.xml 009930.xml 009941.xml 009942.xml 009948.xml 009949.xml 009950.xml 009962.xml "
     ]
    }
   ],
   "source": [
    "# LabelMe type\n",
    "dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "# dataset_name = \"MSPR_Dataset_valset/\"\n",
    "dataset_name = \"MSPR_Dataset_20July/\"\n",
    "new_masks_dir = \"Masks_GT/\"\n",
    "modify_mask_gt = True\n",
    "area_thr = 60\n",
    "objective = \"train\"\n",
    "# objective = \"valid\"\n",
    "datasettype_imported = \"labelme\"\n",
    "\n",
    "# coco_dir = os.path.join(dataset_dir, \"coco/val2014/\")\n",
    "coco_dir = os.path.join(dataset_dir, \"coco/train2014/\")\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################\n",
    "# TO DO, MATCHING AND CALCULATING IOU'S BETWEEN OZAN'S AND FILIZ'S ANNOTATIONS\n",
    "##########################\n",
    "avg_iou = []\n",
    "\n",
    "##########################\n",
    "# ANNOTATIONS OF FILIZ   #\n",
    "##########################\n",
    "filiz_annotations = os.path.join(\"/home/mspr/Desktop/Mask_RCNN/Datasets/MSPR_Dataset/face/filiz_Annotations\",\n",
    "                                 \"pascal_voc_face annotation.txt\")\n",
    "\n",
    "# image_nrs, bbox_filiz = txt_bbox_parser(filiz_annotations)\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "# Added image and annotation ids will start from 600k. \n",
    "image_id = 600000\n",
    "annotation_id = 600000\n",
    "if datasettype_imported == \"labelme\":\n",
    "    for class_name in dict_of_categories.keys():\n",
    "        annot_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Annotations/\")\n",
    "        mask_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Masks/\")\n",
    "        image_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Images/\") \n",
    "\n",
    "        annot_all_files = sorted(os.listdir(os.path.join(dataset_dir, dataset_name, class_name, annot_dir)))\n",
    "        for xml_file in annot_all_files:\n",
    "\n",
    "            tree = ET.parse(os.path.join(annot_dir, xml_file))\n",
    "            root = tree.getroot()\n",
    "\n",
    "            delete_flags = root.findall(\"./object/deleted\")\n",
    "            sum_deleted = sum(map(int, [delete_flag.text for delete_flag in delete_flags]))\n",
    "\n",
    "            xmins = root.findall(\"./object/segm/box/xmin\")\n",
    "            ymins = root.findall(\"./object/segm/box/ymin\")\n",
    "            xmaxs = root.findall(\"./object/segm/box/xmax\")\n",
    "            ymaxs = root.findall(\"./object/segm/box/ymax\")\n",
    "\n",
    "            mask_filenames = root.findall(\"./object/segm/mask\")\n",
    "\n",
    "            # Not yet polygon type of annotations are supported.\n",
    "            assert root.findall(\"./object/polygon\") == [], \"Polygon type of annotations are not yet to be supported.\"\n",
    "\n",
    "            # IMAGES\n",
    "            # Image will be copied to MS COCO train directory and file name will be hold\n",
    "\n",
    "            image_filename = xml_retreiver(root, 'filename')\n",
    "\n",
    "            assert image_filename[-4:] == \".jpg\", \"The filename extension is not .jpg. Name of file: {}\".format(image_filename)\n",
    "            print(xml_file, end=\" \")\n",
    "\n",
    "            image = io.imread(os.path.join(image_dir, image_filename))\n",
    "\n",
    "            if objective == \"train\":\n",
    "                if image_id<1000000:\n",
    "                    image_target_filename = \"COCO_train2014_000000\"+str(image_id)+\".jpg\"\n",
    "                elif (image_id>=1000000 or image_id<=10000000):\n",
    "                    image_target_filename = \"COCO_train2014_00000\"+str(image_id)+\".jpg\"\n",
    "            elif objective == \"valid\":\n",
    "                if image_id<1000000:\n",
    "                    image_target_filename = \"COCO_val2014_000000\"+str(image_id)+\".jpg\"\n",
    "                elif (image_id>=1000000 or image_id<=10000000):\n",
    "                    image_target_filename = \"COCO_val2014_00000\"+str(image_id)+\".jpg\"\n",
    "\n",
    "\n",
    "            io.imsave(os.path.join(coco_dir, image_target_filename), image)\n",
    "\n",
    "            # Image height and width\n",
    "            height = int(xml_retreiver(root, 'nrows'))\n",
    "            width = int(xml_retreiver(root, 'ncols'))\n",
    "\n",
    "            assert height == image.shape[0]\n",
    "            assert width == image.shape[1]\n",
    "\n",
    "\n",
    "            # Miscellaneous metadata\n",
    "            date_captured = DateCaptured()\n",
    "            coco_url = 'http://www.mspr.itu.edu.tr/'\n",
    "            flickr_url = 'http://www.mspr.itu.edu.tr/'\n",
    "            license = np.random.randint(8)\n",
    "\n",
    "            # Appending to 'images'\n",
    "            minival2014['images'].append({'coco_url': coco_url, 'file_name': image_target_filename, \n",
    "                                         'date_captured': date_captured, 'flickr_url': flickr_url,\n",
    "                                         'height': height, 'id': image_id, 'license': license, \n",
    "                                         'width': width})\n",
    "            # Polygon Handling\n",
    "\n",
    "            for d, delete_flag in enumerate(delete_flags):\n",
    "                delete_flag = int(delete_flag.text)\n",
    "                if not delete_flag:\n",
    "\n",
    "                    # ANNOTATIONS\n",
    "                    # Bbox\n",
    "                    bbox_x = int(xmins[d].text)\n",
    "                    bbox_y = int(ymins[d].text)\n",
    "                    bbox_w = int(xmaxs[d].text) - bbox_x\n",
    "                    bbox_h = int(ymaxs[d].text) - bbox_y\n",
    "                    bbox_list = [bbox_x, bbox_y, bbox_w, bbox_h]\n",
    "                    save_bboxes(image_filename[:-4], bbox_list)\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    #                 queries_findid = [d for d, image_nr in enumerate(image_nrs) if image_nr==image_filename[:-4]]\n",
    "    #                 for query in queries_findid:\n",
    "    #                     print(bbox_filiz[query])\n",
    "    #                     print(bbox_list)\n",
    "    #                 iou_for_each_xml_object = [iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid]\n",
    "    #                 avg_iou.append(iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid)    \n",
    "    #                 print(iou_for_each_xml_object)\n",
    "    #                 print(\"\")\n",
    "\n",
    "\n",
    "    #                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "\n",
    "    #                 with open(\"bbox_info.txt\", \"a+\") as f:\n",
    "    #                     f.write(image_filename[:-4]+\",\"+str(bbox_x)+\",\"+str(bbox_y)+\",\"+\n",
    "    #                             str(bbox_x+bbox_w)+\",\"+str(bbox_y)+\",\"+\n",
    "    #                             str(bbox_x)+\",\"+str(bbox_y+bbox_h)+\",\"+\n",
    "    #                            str(bbox_x+bbox_w)+\",\"+str(bbox_y+bbox_h)+\"\\n\")\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "                    # Class\n",
    "                    category_id = dict_of_categories[class_name]\n",
    "\n",
    "                    # Masks Segmentation\n",
    "                    mask = cv2.imread(os.path.join(mask_dir, mask_filenames[d].text), 0)\n",
    "                    polygons, binary_image = mask2poly(mask, morph=modify_mask_gt)\n",
    "\n",
    "                    # Mask should be defined with at least 60 points.\n",
    "                    area_binary_img = len(np.where(binary_image==1)[0])\n",
    "                    if area_binary_img < area_thr:\n",
    "                        continue\n",
    "\n",
    "                    # Write the new modified masks to a folder\n",
    "                    if modify_mask_gt:\n",
    "                        new_mark_dir = os.path.join(dataset_dir, dataset_name, class_name, new_masks_dir)\n",
    "                        cv2.imwrite(os.path.join(new_mark_dir, mask_filenames[d].text), binary_image*255)\n",
    "\n",
    "                    # Generate the RLE version of mask\n",
    "                    RLE_mask = binary_mask_to_rle(mask)\n",
    "                    RLE_mask_coco = comask.encode(np.asfortranarray(binary_image.astype(np.uint8)))\n",
    "\n",
    "                    # Area\n",
    "                    area = comask.area(RLE_mask_coco)\n",
    "\n",
    "                    #  Iscrowd\n",
    "                    if (len(mask_filenames) - sum_deleted) > 1:\n",
    "                        iscrowd = 1\n",
    "                        minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                           'image_id': image_id, 'segmentation': RLE_mask,\n",
    "                                                           'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "                    else:\n",
    "                        iscrowd = 0\n",
    "                        minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                      'image_id': image_id, 'segmentation': polygons,\n",
    "                                                      'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "\n",
    "                    # Appending into annotations\n",
    "\n",
    "                    annotation_id+=1\n",
    "            image_id+=1\n",
    "\n",
    "                \n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "# print(\"Average IoU = {}\".format(sum(avg_iou)/len(avg_iou)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in memory and then writes them into a file. \n",
    "\n",
    "# from decimal import Decimal\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "with open(\"Datasets/coco/annotations/instances_train2014.json\",\"w\") as f:\n",
    "    \n",
    "# with open(\"Datasets/coco/annotations/instances_minival2014.json\",\"w\") as f:\n",
    "    data = json.dumps(minival2014, cls=MyEncoder, indent=4)\n",
    "    f.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
