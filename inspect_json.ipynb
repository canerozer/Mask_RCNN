{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import os\n",
    "from pycocotools import coco\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml\n",
    "from pycocotools import mask as comask\n",
    "import cv2\n",
    "from utils import DateCaptured, coco_bbox_creator, iou_calculator, mask2poly, binary_mask_to_rle\n",
    "import datetime\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelMe type preset\n",
    "dataset_dir = \"/home/dontgetdown/Desktop/Mask_RCNN/Datasets\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "dataset_name = \"MSPR_Dataset/\"\n",
    "new_masks_dir = \"Masks_GT/\"\n",
    "modify_mask_gt = True\n",
    "area_thr = 60\n",
    "datasettype_imported = \"labelme\"\n",
    "\n",
    "# objective = \"train\"\n",
    "objective = \"valid\"\n",
    "\n",
    "coco_dir = os.path.join(dataset_dir, \"coco/val2014/\")\n",
    "# coco_dir = os.path.join(dataset_dir, \"coco/train2014/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CATEGORY INFORMATION\n",
    "\n",
    "# Add the name of the folders inside the dataset and\n",
    "# assign some label id. Start iding from 91.\n",
    "\n",
    "dict_of_categories = {'face': 91}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET INFORMATION\n",
    "\n",
    "# Enter the original/custom JSON file to be used.\n",
    "# I used the original train2014 JSON file containing\n",
    "# the instances.\n",
    "\n",
    "# json_dir = \"Datasets/coco/annotations_old/instances_train2014.json\"\n",
    "\n",
    "# for the purpose of creating validation images json\n",
    "# json_dir = \"Datasets/coco/annotations_old/instances_val2014.json\" \n",
    "\n",
    "# for the purpose of creating testing images json\n",
    "json_dir = \"Datasets/coco/annotations_old/instances_minival2014.json\" \n",
    "\n",
    "### For viewing the content of the new json file. ###\n",
    "# json_dir = \"Datasets/coco/annotations/instances_train2014.json\"\n",
    "\n",
    "### minival2014 will become the main dictionary ###\n",
    "minival2014 = json.load(open(json_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEARING THE DATASET\n",
    "# Use that if you only going to make a dataset of your own.\n",
    "# If you are going to append the images, this operation is not necessary.\n",
    "\n",
    "minival2014['images'] = []\n",
    "minival2014['annotations'] = []\n",
    "minival2014['categories'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the number of instances per class and gathering some statistics about them.\n",
    "# labels = np.zeros((91, ), dtype=np.int32)\n",
    "# for annotation in minival2014['annotations']:\n",
    "#     labels[annotation['category_id']] += 1\n",
    "# used_label_ids = np.where(labels!=0)\n",
    "# ex_labels = labels[used_label_ids]\n",
    "# print(ex_labels)\n",
    "# print(\"Average instances per label: {}\".format(sum(ex_labels)/81))\n",
    "# print(\"Maximum instances per label: {}\".format(max(ex_labels)))\n",
    "# print(\"Minimum instances per label: {}\".format(min(ex_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPENDING CLASSES TO THE CATEGORIES\n",
    "\n",
    "### For viewing the categories inside the JSON file.\n",
    "# minival2014['categories']\n",
    "\n",
    "# Add the classes based on to the dict_of_categories\n",
    "# Below shows an example for adding the face class to the main dictionary. \n",
    "\n",
    "minival2014['categories'].append({'id': 91, 'name': 'face', 'supercategory': 'person'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to be used:\n",
    "def isjpg(string):\n",
    "    if string[-4:]==\".jpg\":\n",
    "        return True \n",
    "    \n",
    "def ispng(string):\n",
    "    if string[-4:]==\".png\":\n",
    "        return True\n",
    "    \n",
    "def save_bboxes(image_name, bbox_coor):\n",
    "    with open('img_bbox.txt','a+') as file:\n",
    "        file.write(str(image_name)+','+str(bbox_coor[0])+','+str(bbox_coor[1])+','+str(bbox_coor[2])+','+str(bbox_coor[3])+'\\n')\n",
    "\n",
    "def xml_retreiver(tree_object, key_name):\n",
    "    for iterator in tree_object.iter(key_name):\n",
    "        return iterator.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000001.xml 000021.xml 000025.xml "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/morphology/misc.py:122: UserWarning: Only one label was provided to `remove_small_objects`. Did you mean to use a boolean array?\n",
      "  warn(\"Only one label was provided to `remove_small_objects`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000027.xml 000030.xml 000035.xml 000038.xml 000043.xml 000050.xml 000051.xml 000058.xml 000069.xml 000073.xml 000076.xml 000081.xml 000085.xml 000089.xml 000090.xml 000096.xml 000101.xml 000104.xml 000105.xml 000113.xml 000124.xml 000125.xml 000127.xml 000129.xml 000133.xml 000138.xml 000139.xml 000144.xml 000146.xml 000151.xml 000159.xml 000162.xml 000164.xml 000165.xml 000166.xml 000169.xml 000170.xml 000171.xml 000173.xml 000174.xml 000177.xml 000181.xml 000182.xml 000191.xml 000192.xml 000193.xml 000200.xml 000202.xml 000205.xml 000206.xml 000212.xml 000220.xml 000222.xml 000229.xml 000230.xml 000231.xml 000237.xml 000245.xml 000247.xml 000248.xml 000252.xml 000257.xml 000258.xml 000259.xml 000264.xml 000265.xml 000269.xml 000271.xml 000272.xml 000275.xml 000276.xml 000278.xml 000280.xml 000282.xml 000283.xml 000285.xml 000286.xml 000287.xml 000297.xml 000298.xml 000299.xml 000302.xml 000305.xml 000308.xml 000310.xml 000315.xml 000319.xml 000320.xml 000321.xml 000322.xml 000323.xml 000328.xml 000331.xml 000337.xml 000339.xml 000342.xml 000346.xml 000348.xml 000352.xml 000356.xml 000359.xml 000367.xml 000368.xml 000369.xml 000372.xml 000374.xml 000377.xml 000378.xml 000386.xml 000388.xml 000392.xml 000393.xml 000394.xml 000405.xml 000407.xml 000409.xml 000413.xml 000414.xml 000423.xml 000428.xml 000433.xml 000434.xml 000438.xml 000446.xml 000448.xml 000449.xml 000453.xml 000457.xml 000468.xml 000476.xml 000477.xml 000479.xml 000483.xml 000485.xml 000490.xml 000493.xml 000497.xml 000498.xml 000499.xml 000500.xml 000502.xml 000506.xml 000507.xml 000516.xml 000517.xml 000520.xml 000523.xml 000524.xml 000526.xml 000530.xml 000531.xml 000534.xml 000535.xml 000539.xml 000545.xml 000546.xml 000555.xml 000562.xml 000566.xml 000567.xml 000578.xml 000586.xml 000587.xml 000589.xml 000594.xml 000602.xml 000604.xml 000606.xml 000612.xml 000613.xml 000615.xml 000616.xml 000617.xml 000624.xml 000633.xml 000639.xml 000641.xml 000642.xml 000643.xml 000644.xml 000652.xml 000662.xml 000664.xml 000670.xml 000677.xml 000683.xml 000828.xml 000839.xml 000843.xml 000847.xml 000848.xml 000851.xml 000854.xml 000856.xml 000859.xml 000860.xml 000861.xml 000865.xml 000870.xml 000874.xml 000878.xml 000879.xml 000886.xml 000892.xml 000895.xml 000903.xml 000909.xml 000910.xml 000915.xml 000916.xml 000918.xml 000922.xml 000926.xml 000940.xml 000943.xml 000952.xml 000955.xml 000956.xml 000959.xml 000969.xml 000978.xml 000981.xml 000984.xml 000987.xml 000988.xml 000999.xml 001014.xml 001020.xml 001021.xml 001024.xml 001026.xml 001028.xml 001033.xml 001036.xml 001037.xml 001038.xml 001040.xml 001047.xml 001055.xml 001061.xml 001065.xml 001067.xml 001071.xml 001072.xml 001079.xml 001086.xml 001091.xml 001095.xml 001097.xml 001105.xml 001108.xml 001109.xml 001113.xml 001116.xml 001118.xml 001129.xml 001133.xml 001137.xml 001140.xml 001147.xml 001150.xml 001164.xml 001167.xml 001170.xml 001173.xml 001185.xml 001220.xml 001228.xml 001229.xml 001234.xml 001236.xml 001242.xml 001243.xml 001244.xml 001251.xml 001253.xml 001261.xml 001265.xml 001272.xml 001279.xml 001282.xml 001284.xml 001287.xml 001297.xml 001307.xml 001309.xml 001310.xml 001311.xml 001315.xml 001319.xml 001320.xml 001325.xml 001327.xml 001330.xml 001333.xml 001336.xml 001337.xml 001340.xml 001346.xml 001347.xml 001350.xml 001351.xml 001353.xml 001354.xml 001358.xml 001362.xml 001366.xml 001368.xml 001370.xml 001376.xml 001388.xml 001390.xml 001393.xml 001406.xml 001408.xml 001409.xml 001411.xml 001417.xml 001419.xml 001493.xml 001495.xml 001496.xml 001498.xml 001502.xml 001514.xml 001516.xml 001521.xml 001523.xml 001526.xml 001531.xml 001533.xml 001537.xml 001544.xml 001548.xml 001554.xml 001558.xml 001561.xml 001563.xml 001564.xml 001566.xml 001569.xml 001570.xml 001571.xml 001579.xml 001580.xml 001581.xml 001583.xml 001585.xml 001593.xml 001606.xml 001608.xml 001611.xml 001620.xml 001627.xml 001629.xml 001630.xml 001644.xml 001649.xml 001651.xml 001652.xml 001657.xml 001682.xml 001686.xml 001689.xml 001710.xml 001712.xml 001730.xml 001735.xml 001743.xml 001749.xml 001754.xml 001756.xml 001757.xml 001759.xml 001760.xml 001769.xml 001776.xml 001781.xml 001784.xml 001785.xml 001791.xml 001792.xml 001793.xml 001794.xml 001797.xml 001800.xml 001802.xml 001805.xml 001808.xml 001810.xml 001812.xml 001828.xml "
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################\n",
    "# TO DO, MATCHING AND CALCULATING IOU'S BETWEEN OZAN'S AND FILIZ'S ANNOTATIONS\n",
    "##########################\n",
    "avg_iou = []\n",
    "\n",
    "##########################\n",
    "# ANNOTATIONS OF FILIZ   #\n",
    "##########################\n",
    "filiz_annotations = os.path.join(\"/home/mspr/Desktop/Mask_RCNN/Datasets/MSPR_Dataset/face/filiz_Annotations\",\n",
    "                                 \"pascal_voc_face annotation.txt\")\n",
    "\n",
    "# image_nrs, bbox_filiz = txt_bbox_parser(filiz_annotations)\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "# Added image and annotation ids will start from 600k. \n",
    "image_id = 600000\n",
    "annotation_id = 600000\n",
    "if datasettype_imported == \"labelme\":\n",
    "    for class_name in dict_of_categories.keys():\n",
    "        annot_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Annotations/\")\n",
    "        mask_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Masks/\")\n",
    "        image_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Images/\") \n",
    "\n",
    "        annot_all_files = sorted(os.listdir(os.path.join(dataset_dir, dataset_name, class_name, annot_dir)))\n",
    "        for xml_file in annot_all_files:\n",
    "\n",
    "            tree = ET.parse(os.path.join(annot_dir, xml_file))\n",
    "            root = tree.getroot()\n",
    "\n",
    "            delete_flags = root.findall(\"./object/deleted\")\n",
    "            sum_deleted = sum(map(int, [delete_flag.text for delete_flag in delete_flags]))\n",
    "\n",
    "            xmins = root.findall(\"./object/segm/box/xmin\")\n",
    "            ymins = root.findall(\"./object/segm/box/ymin\")\n",
    "            xmaxs = root.findall(\"./object/segm/box/xmax\")\n",
    "            ymaxs = root.findall(\"./object/segm/box/ymax\")\n",
    "\n",
    "            mask_filenames = root.findall(\"./object/segm/mask\")\n",
    "\n",
    "            # Not yet polygon type of annotations are supported.\n",
    "            assert root.findall(\"./object/polygon\") == [], \"Polygon type of annotations are not yet to be supported.\"\n",
    "\n",
    "            # IMAGES\n",
    "            # Image will be copied to MS COCO train directory and file name will be hold\n",
    "\n",
    "            image_filename = xml_retreiver(root, 'filename')\n",
    "\n",
    "            assert image_filename[-4:] == \".jpg\", \"The filename extension is not .jpg. Name of file: {}\".format(image_filename)\n",
    "            print(xml_file, end=\" \")\n",
    "\n",
    "            image = io.imread(os.path.join(image_dir, image_filename))\n",
    "\n",
    "            if objective == \"train\":\n",
    "                if image_id<1000000:\n",
    "                    image_target_filename = \"COCO_train2014_000000\"+str(image_id)+\".jpg\"\n",
    "                elif (image_id>=1000000 or image_id<=10000000):\n",
    "                    image_target_filename = \"COCO_train2014_00000\"+str(image_id)+\".jpg\"\n",
    "            elif objective == \"valid\":\n",
    "                if image_id<1000000:\n",
    "                    image_target_filename = \"COCO_val2014_000000\"+str(image_id)+\".jpg\"\n",
    "                elif (image_id>=1000000 or image_id<=10000000):\n",
    "                    image_target_filename = \"COCO_val2014_00000\"+str(image_id)+\".jpg\"\n",
    "\n",
    "\n",
    "            io.imsave(os.path.join(coco_dir, image_target_filename), image)\n",
    "\n",
    "            # Image height and width\n",
    "            height = int(xml_retreiver(root, 'nrows'))\n",
    "            width = int(xml_retreiver(root, 'ncols'))\n",
    "\n",
    "            assert height == image.shape[0]\n",
    "            assert width == image.shape[1]\n",
    "\n",
    "\n",
    "            # Miscellaneous metadata\n",
    "            date_captured = DateCaptured()\n",
    "            coco_url = 'http://www.mspr.itu.edu.tr/'\n",
    "            flickr_url = 'http://www.mspr.itu.edu.tr/'\n",
    "            license = np.random.randint(8)\n",
    "\n",
    "            # Appending to 'images'\n",
    "            minival2014['images'].append({'coco_url': coco_url, 'file_name': image_target_filename, \n",
    "                                         'date_captured': date_captured, 'flickr_url': flickr_url,\n",
    "                                         'height': height, 'id': image_id, 'license': license, \n",
    "                                         'width': width})\n",
    "            # Polygon Handling\n",
    "\n",
    "            for d, delete_flag in enumerate(delete_flags):\n",
    "                delete_flag = int(delete_flag.text)\n",
    "                if not delete_flag:\n",
    "\n",
    "                    # ANNOTATIONS\n",
    "                    # Bbox\n",
    "                    bbox_x = int(xmins[d].text)\n",
    "                    bbox_y = int(ymins[d].text)\n",
    "                    bbox_w = int(xmaxs[d].text) - bbox_x\n",
    "                    bbox_h = int(ymaxs[d].text) - bbox_y\n",
    "                    bbox_list = [bbox_x, bbox_y, bbox_w, bbox_h]\n",
    "                    save_bboxes(image_filename[:-4], bbox_list)\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "    #                 queries_findid = [d for d, image_nr in enumerate(image_nrs) if image_nr==image_filename[:-4]]\n",
    "    #                 for query in queries_findid:\n",
    "    #                     print(bbox_filiz[query])\n",
    "    #                     print(bbox_list)\n",
    "    #                 iou_for_each_xml_object = [iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid]\n",
    "    #                 avg_iou.append(iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid)    \n",
    "    #                 print(iou_for_each_xml_object)\n",
    "    #                 print(\"\")\n",
    "\n",
    "\n",
    "    #                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "\n",
    "    #                 with open(\"bbox_info.txt\", \"a+\") as f:\n",
    "    #                     f.write(image_filename[:-4]+\",\"+str(bbox_x)+\",\"+str(bbox_y)+\",\"+\n",
    "    #                             str(bbox_x+bbox_w)+\",\"+str(bbox_y)+\",\"+\n",
    "    #                             str(bbox_x)+\",\"+str(bbox_y+bbox_h)+\",\"+\n",
    "    #                            str(bbox_x+bbox_w)+\",\"+str(bbox_y+bbox_h)+\"\\n\")\n",
    "\n",
    "    ##########################################################################\n",
    "    ##########################################################################\n",
    "\n",
    "                    # Class\n",
    "                    category_id = dict_of_categories[class_name]\n",
    "\n",
    "                    # Masks Segmentation\n",
    "                    mask = cv2.imread(os.path.join(mask_dir, mask_filenames[d].text), 0)\n",
    "                    polygons, binary_image = mask2poly(mask, modify=modify_mask_gt)\n",
    "\n",
    "                    # Mask should be defined with at least 60 points.\n",
    "                    area_binary_img = len(np.where(binary_image==1)[0])\n",
    "                    if area_binary_img < area_thr:\n",
    "                        continue\n",
    "\n",
    "                    # Write the new modified masks to a folder\n",
    "                    if modify_mask_gt:\n",
    "                        new_mark_dir = os.path.join(dataset_dir, dataset_name, class_name, new_masks_dir)\n",
    "                        cv2.imwrite(os.path.join(new_mark_dir, mask_filenames[d].text), binary_image*255)\n",
    "\n",
    "                    # Generate the RLE version of mask\n",
    "                    RLE_mask = binary_mask_to_rle(mask)\n",
    "                    RLE_mask_coco = comask.encode(np.asfortranarray(binary_image.astype(np.uint8)))\n",
    "\n",
    "                    # Area\n",
    "                    area = comask.area(RLE_mask_coco)\n",
    "\n",
    "                    #  Iscrowd\n",
    "                    if (len(mask_filenames) - sum_deleted) > 1:\n",
    "                        iscrowd = 1\n",
    "                        minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                           'image_id': image_id, 'segmentation': RLE_mask,\n",
    "                                                           'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "                    else:\n",
    "                        iscrowd = 0\n",
    "                        minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                      'image_id': image_id, 'segmentation': polygons,\n",
    "                                                      'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "\n",
    "                    # Appending into annotations\n",
    "\n",
    "                    annotation_id+=1\n",
    "            image_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in memory and then writes them into a file. \n",
    "\n",
    "# from decimal import Decimal\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "# with open(\"Datasets/coco/annotations/instances_train2014.json\",\"w\") as f:\n",
    "    \n",
    "with open(\"Datasets/coco/annotations/instances_minival2014.json\",\"w\") as f:\n",
    "    data = json.dumps(minival2014, cls=MyEncoder, indent=4)\n",
    "    f.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
