{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import datetime\n",
    "import os\n",
    "from pycocotools import coco\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml\n",
    "from pycocotools import mask as comask\n",
    "import cv2\n",
    "from itertools import groupby\n",
    "from utils import DateCaptured, coco_bbox_creator, xml_retreiver, iou_calculator, biggest_segment, mask2poly, binary_mask_to_rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CATEGORY INFORMATION\n",
    "\n",
    "# Add the name of the folders inside the dataset and\n",
    "# assign some label id. Start iding from 91.\n",
    "\n",
    "dict_of_categories = {'face': 91}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASET INFORMATION\n",
    "\n",
    "# Enter the original/custom JSON file to be used.\n",
    "# I used the original train2014 JSON file containing\n",
    "# the instances.\n",
    "\n",
    "json_dir = \"Datasets/coco/annotations_old/instances_train2014.json\"\n",
    "\n",
    "### For viewing the content of the new json file. ###\n",
    "# json_dir = \"Datasets/coco/annotations/instances_train2014.json\"\n",
    "\n",
    "### minival2014 will become the main dictionary ###\n",
    "minival2014 = json.load(open(json_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEARING THE DATASET\n",
    "# Use that if you only going to make a dataset of your own.\n",
    "# If you are going to append the images, this operation is not necessary.\n",
    "\n",
    "# minival2014['images'] = []\n",
    "# minival2014['annotations'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPENDING CLASSES TO THE CATEGORIES\n",
    "\n",
    "### For viewing the categories inside the JSON file.\n",
    "# minival2014['categories']\n",
    "\n",
    "# Add the classes based on to the dict_of_categories\n",
    "# Below shows an example for adding the face class to the main dictionary. \n",
    "\n",
    "minival2014['categories'].append({'id': 91, 'name': 'face', 'supercategory': 'person'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to be used:\n",
    "def isjpg(string):\n",
    "    if string[-4:]==\".jpg\":\n",
    "        return True \n",
    "    \n",
    "def ispng(string):\n",
    "    if string[-4:]==\".png\":\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelMe type\n",
    "dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "dataset_name = \"MSPR_Dataset/\"\n",
    "\n",
    "target_dataset_name = os.path.join(dataset_dir, \"coco/train2014/\")\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################\n",
    "# TO DO, MATCHING AND CALCULATING IOU'S BETWEEN OZAN'S AND FILIZ'S ANNOTATIONS\n",
    "##########################\n",
    "avg_iou = []\n",
    "\n",
    "##########################\n",
    "# ANNOTATIONS OF FILIZ   #\n",
    "##########################\n",
    "filiz_annotations = os.path.join(\"/home/mspr/Desktop/Mask_RCNN/Datasets/MSPR_Dataset/face/filiz_Annotations\",\n",
    "                                 \"pascal_voc_face annotation.txt\")\n",
    "\n",
    "# image_nrs, bbox_filiz = txt_bbox_parser(filiz_annotations)\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "# Added image and annotation ids will start from 600k. \n",
    "image_id = 600000\n",
    "annotation_id = 600000\n",
    "\n",
    "for class_name in dict_of_categories.keys():\n",
    "    annot_all_files = sorted(os.listdir(os.path.join(dataset_dir, dataset_name, class_name, annot_dir)))\n",
    "    for xml_file in annot_all_files:\n",
    "        annot_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Annotations/\")\n",
    "        mask_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Masks/\")\n",
    "        image_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Images/\")\n",
    "        new_mark_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Masks_GT/\")\n",
    "\n",
    "        tree = ET.parse(os.path.join(annot_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        delete_flags = root.findall(\"./object/deleted\")\n",
    "        sum_deleted = sum(map(int, [delete_flag.text for delete_flag in delete_flags]))\n",
    "\n",
    "        xmins = root.findall(\"./object/segm/box/xmin\")\n",
    "        ymins = root.findall(\"./object/segm/box/ymin\")\n",
    "        xmaxs = root.findall(\"./object/segm/box/xmax\")\n",
    "        ymaxs = root.findall(\"./object/segm/box/ymax\")\n",
    "\n",
    "        mask_filenames = root.findall(\"./object/segm/mask\")\n",
    "        # IMAGES\n",
    "        # Image will be copied to MS COCO train directory and file name will be hold\n",
    "        image_filename = xml_retreiver(root, 'filename')\n",
    "        image = io.imread(os.path.join(image_dir, image_filename))\n",
    "        if image_id<1000000:\n",
    "            image_target_filename = \"COCO_train2014_000000\"+str(image_id)+\".jpg\"\n",
    "        elif (image_id>=1000000 or image_id<=10000000):\n",
    "            image_target_filename = \"COCO_train2014_00000\"+str(image_id)+\".jpg\"\n",
    "\n",
    "# Do not rewrite the image if something exists\n",
    "###TODO: read the image and rewrite it if the content and target are not the same\n",
    "#         if os.path.isfile(os.path.join(target_dataset_name, image_target_filename))==False:\n",
    "#             io.imsave(os.path.join(target_dataset_name, image_target_filename), image)\n",
    "        \n",
    "        io.imsave(os.path.join(target_dataset_name, image_target_filename), image)\n",
    "\n",
    "        # Image height and width\n",
    "        height = int(xml_retreiver(root, 'nrows'))\n",
    "        width = int(xml_retreiver(root, 'ncols'))\n",
    "\n",
    "        assert height == image.shape[0]\n",
    "        assert width == image.shape[1]\n",
    "\n",
    "\n",
    "        # Miscellaneous metadata\n",
    "        date_captured = DateCaptured()\n",
    "        coco_url = 'n/a'\n",
    "        flickr_url = 'n/a'\n",
    "        license = np.random.randint(8)\n",
    "\n",
    "        # Appending to 'images'\n",
    "        minival2014['images'].append({'coco_url': coco_url, 'file_name': image_target_filename, \n",
    "                                     'date_captured': date_captured, 'flickr_url': flickr_url,\n",
    "                                     'height': height, 'id': image_id, 'license': license, \n",
    "                                     'width': width})\n",
    "        # Polygon Handling\n",
    "\n",
    "        for d, delete_flag in enumerate(delete_flags):\n",
    "            delete_flag = int(delete_flag.text)\n",
    "            if not delete_flag:\n",
    "\n",
    "\n",
    "                # ANNOTATIONS\n",
    "                # Bbox\n",
    "                bbox_x = int(xmins[d].text)\n",
    "                bbox_y = int(ymins[d].text)\n",
    "                bbox_w = int(xmaxs[d].text) - bbox_x\n",
    "                bbox_h = int(ymaxs[d].text) - bbox_y\n",
    "                bbox_list = [bbox_x, bbox_y, bbox_w, bbox_h]\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "#                 queries_findid = [d for d, image_nr in enumerate(image_nrs) if image_nr==image_filename[:-4]]\n",
    "#                 for query in queries_findid:\n",
    "#                     print(bbox_filiz[query])\n",
    "#                     print(bbox_list)\n",
    "#                 iou_for_each_xml_object = [iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid]\n",
    "#                 avg_iou.append(iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid)    \n",
    "#                 print(iou_for_each_xml_object)\n",
    "#                 print(\"\")\n",
    "\n",
    "\n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "\n",
    "#                 with open(\"bbox_info.txt\", \"a+\") as f:\n",
    "#                     f.write(image_filename[:-4]+\",\"+str(bbox_x)+\",\"+str(bbox_y)+\",\"+\n",
    "#                             str(bbox_x+bbox_w)+\",\"+str(bbox_y)+\",\"+\n",
    "#                             str(bbox_x)+\",\"+str(bbox_y+bbox_h)+\",\"+\n",
    "#                            str(bbox_x+bbox_w)+\",\"+str(bbox_y+bbox_h)+\"\\n\")\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "                # Class\n",
    "                category_id = dict_of_categories[class_name]\n",
    "\n",
    "                # Masks Segmentation\n",
    "                instance_seg_info = []\n",
    "                mask = cv2.imread(os.path.join(mask_dir, mask_filenames[d].text), 0)\n",
    "                polygons, binary_image = mask2poly(mask)\n",
    "                cv2.imwrite(os.path.join(new_mark_dir, mask_filenames[d].text), binary_image*255)\n",
    "                RLE_mask = binary_mask_to_rle(mask)\n",
    "                RLE_mask_coco = comask.encode(np.asfortranarray(binary_image))\n",
    "\n",
    "                # Area\n",
    "                area = comask.area(RLE_mask_coco)\n",
    "\n",
    "                #  Iscrowd\n",
    "                if (len(mask_filenames) - sum_deleted) > 1:\n",
    "                    iscrowd = 1\n",
    "                    minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                       'image_id': image_id, 'segmentation': RLE_mask,\n",
    "                                                       'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "                else:\n",
    "                    iscrowd = 0\n",
    "                    minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                  'image_id': image_id, 'segmentation': polygons,\n",
    "                                                  'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "\n",
    "                # Appending into annotations\n",
    "                \n",
    "                annotation_id+=1\n",
    "        image_id+=1\n",
    "\n",
    "                \n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "# print(\"Average IoU = {}\".format(sum(avg_iou)/len(avg_iou)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in memory and then writes them into a file. \n",
    "\n",
    "# from decimal import Decimal\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "with open(\"Datasets/coco/annotations/instances_train2014.json\",\"w\") as f:\n",
    "# with open(\"Datasets/coco/annotations/instances_valminusminival2014.json\",\"w\") as f:\n",
    "    data = json.dumps(minival2014, cls=MyEncoder, indent=4)\n",
    "    f.write(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
