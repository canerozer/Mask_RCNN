{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import datetime\n",
    "import os\n",
    "from pycocotools import coco\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml\n",
    "from pycocotools import mask as comask\n",
    "import cv2\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CATEGORY INFORMATION\n",
    "dict_of_categories = {'face': 91}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coco_url': 'http://images.cocodataset.org/train2014/COCO_train2014_000000475546.jpg',\n",
       " 'date_captured': '2013-11-25 21:20:23',\n",
       " 'file_name': 'COCO_train2014_000000475546.jpg',\n",
       " 'flickr_url': 'http://farm1.staticflickr.com/167/423175046_6cd9d0205a_z.jpg',\n",
       " 'height': 375,\n",
       " 'id': 475546,\n",
       " 'license': 4,\n",
       " 'width': 500}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dir = \"Datasets/coco/annotations_old/instances_train2014.json\"\n",
    "\n",
    "# For viewing the content of the new json file.\n",
    "# json_dir = \"Datasets/coco/annotations/instances_train2014.json\"\n",
    "\n",
    "minival2014 = json.load(open(json_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minival2014['images'] = []\n",
    "# minival2014['annotations'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the classes, view the category list at first!!\n",
    "minival2014['categories'].append({'id': 91, 'name': 'face', 'supercategory': 'person'})\n",
    "# minival2014['categories'].append({'id': 92, 'name': 'fish', 'supercategory': 'animal'})\n",
    "# minival2014['categories']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to be used:\n",
    "def isjpg(string):\n",
    "    if string[-4:]==\".jpg\":\n",
    "        return True \n",
    "    \n",
    "def ispng(string):\n",
    "    if string[-4:]==\".png\":\n",
    "        return True \n",
    "    \n",
    "def DateCaptured():\n",
    "    dt = str(datetime.datetime.now())\n",
    "    date, time = dt.split()\n",
    "    time = time.split(\".\")[0]\n",
    "    return date+\" \"+time\n",
    "\n",
    "def coco_bbox_creator(x, y):\n",
    "    x = list(map(lambda x: float(x), x))\n",
    "    y = list(map(lambda x: float(x), y))\n",
    "    x_min = min(x)\n",
    "    y_min = min(y)\n",
    "    w = max(x) - x_min\n",
    "    h = max(y) - y_min\n",
    "    return [x_min, y_min, w, h]\n",
    "\n",
    "def PolyArea(x,y):\n",
    "    \"\"\"\n",
    "    Parameters:   x, y: The inputs are representing the nd-array of coordinates\n",
    "    of a polygon.\n",
    "    \n",
    "    Returns: The area of the polygon\n",
    "    \"\"\"\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def xml_retreiver(tree_object, key_name):\n",
    "    for iterator in tree_object.iter(key_name):\n",
    "        return iterator.text\n",
    "    \n",
    "def iou_calculator(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Parameters:   bbox1, bbox2: list or numpy array of bounding box coordinates.\n",
    "    The input should contain the top-left corner's x and y coordinates and \n",
    "    width and height of the bounding boxes.\n",
    "    \n",
    "    Assertations: width and height informations of bbox1 and bbox2 should be \n",
    "    larger than 0.\n",
    "    \n",
    "    Returns:      iou: A floating point decimal representing the IoU ratio, which\n",
    "    is the division of bounding box areas of intersection to their union.\n",
    "    \"\"\"\n",
    "    x1, y1, w1, h1 = bbox1\n",
    "    x2, y2, w2, h2 = bbox2\n",
    "    assert w1 and w2 > 0\n",
    "    assert w1 and h2 > 0\n",
    "    \n",
    "    iou = 0\n",
    "    if (((x1>x2 and x1<x2+w2) or (x1+w1>x2 and x1+w1<x2+w2) or \n",
    "        (x2>x1 and x2<x1+w1) or (x2+w2>x1 and x2+w2<x1+w1)) and \n",
    "        ((y1>y2 and y1<y2+h2) or (y1+h1>y2 and y1+h1<y2+h2) or\n",
    "        (y2>y1 and y2<y1+h1) or (y2+h2>y1 and y2+h2<y1+h1))):\n",
    "        iou_xmin = float(max(x1, x2))\n",
    "        iou_xmax = float(min(x1+w1, x2+w2))\n",
    "        iou_ymin = float(max(y1, y2))\n",
    "        iou_ymax = float(min(y1+h1, y2+h2))\n",
    "        intersection_area = (iou_ymax - iou_ymin)*(iou_xmax - iou_xmin)\n",
    "        total_area = float(w1)*float(h1) + float(w2)*float(h2) - intersection_area\n",
    "        iou = intersection_area/total_area\n",
    "    return iou\n",
    "\n",
    "def txt_bbox_parser(input_location):\n",
    "    \"\"\"\n",
    "    Parameters: input_location: The input will be a text file denoting the bounding\n",
    "    boxes for every frame in such format:\n",
    "    frame_id, x1, y1, x2, y2, x3, y3, x4, y4\n",
    "    \n",
    "    Returns: image_nr: A list containing the the frame numbers. \n",
    "             xywh: A list containing the bounding boxes in (x1, y1, w, h) format,\n",
    "                   where w and h are the width and height of a bounding box respectively.\n",
    "    \"\"\"\n",
    "    with open(input_location) as f:\n",
    "        bboxes = f.readlines()\n",
    "    image_nr = []\n",
    "    xywh = []\n",
    "    for d, bbox in enumerate(bboxes):\n",
    "        image_nr.append(bbox.split(\",\")[0])\n",
    "        coords = np.array(bbox.split(\",\")[1:]).reshape((-1, 2))\n",
    "        x, y = coords[:,0], coords[:,1]\n",
    "        xywh.append(coco_bbox_creator(x, y))\n",
    "    return image_nr, xywh\n",
    "\n",
    "def mask2poly(mask):\n",
    "    _, mask = cv2.threshold(mask,1,1,cv2.THRESH_BINARY)  #threshold binary image\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    _,countours,_ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    polygons = []\n",
    "    for countour in countours:\n",
    "        if countour.size >=6:\n",
    "            polygons.append(countour.flatten().tolist())\n",
    "            \n",
    "    return polygons, mask\n",
    "\n",
    "def binary_mask_to_rle(binary_mask):\n",
    "    rle = {'counts': [], 'size': list(binary_mask.shape)}\n",
    "    counts = rle.get('counts')\n",
    "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n",
    "        if i == 0 and value == 1:\n",
    "                counts.append(0)\n",
    "        counts.append(len(list(elements)))\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# LabelMe type\n",
    "dataset_dir = \"/home/mspr/Desktop/Mask_RCNN/Datasets/\"  # Will be changed with argparse, includes directories for video names and corresponding frames inside\n",
    "dataset_name = \"MSPR_Dataset/\"\n",
    "annot_dir = \"Annotations/\"\n",
    "mask_dir = \"Masks/\"\n",
    "image_dir = \"Images/\"\n",
    "\n",
    "target_dataset_name = os.path.join(dataset_dir, \"coco/train2014/\")\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################\n",
    "# TO DO, MATCHING AND CALCULATING IOU'S BETWEEN OZAN'S AND FILIZ'S ANNOTATIONS\n",
    "##########################\n",
    "avg_iou = []\n",
    "\n",
    "##########################\n",
    "# ANNOTATIONS OF FILIZ   #\n",
    "##########################\n",
    "filiz_annotations = os.path.join(\"/home/mspr/Desktop/Mask_RCNN/Datasets/MSPR_Dataset/face/filiz_Annotations\",\n",
    "                                 \"pascal_voc_face annotation.txt\")\n",
    "\n",
    "image_nrs, bbox_filiz = txt_bbox_parser(filiz_annotations)\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "# Added image and annotation ids will start from 600k. \n",
    "image_id = 600000\n",
    "annotation_id = 600000\n",
    "\n",
    "for class_name in dict_of_categories.keys():\n",
    "    annot_all_files = sorted(os.listdir(os.path.join(dataset_dir, dataset_name, class_name, annot_dir)))\n",
    "    for xml_file in annot_all_files:\n",
    "        annot_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Annotations/\")\n",
    "        mask_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Masks/\")\n",
    "        image_dir = os.path.join(dataset_dir, dataset_name, class_name, \"Images/\")\n",
    "\n",
    "        tree = ET.parse(os.path.join(annot_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        delete_flags = root.findall(\"./object/deleted\")\n",
    "        sum_deleted = sum(map(int, [delete_flag.text for delete_flag in delete_flags]))\n",
    "\n",
    "        xmins = root.findall(\"./object/segm/box/xmin\")\n",
    "        ymins = root.findall(\"./object/segm/box/ymin\")\n",
    "        xmaxs = root.findall(\"./object/segm/box/xmax\")\n",
    "        ymaxs = root.findall(\"./object/segm/box/ymax\")\n",
    "\n",
    "        mask_filenames = root.findall(\"./object/segm/mask\")\n",
    "        # IMAGES\n",
    "        # Image will be copied to MS COCO train directory and file name will be hold\n",
    "        image_filename = xml_retreiver(root, 'filename')\n",
    "        image = io.imread(os.path.join(image_dir, image_filename))\n",
    "        if image_id<1000000:\n",
    "            image_target_filename = \"COCO_train2014_000000\"+str(image_id)+\".jpg\"\n",
    "        elif (image_id>=1000000 or image_id<=10000000):\n",
    "            image_target_filename = \"COCO_train2014_00000\"+str(image_id)+\".jpg\"\n",
    "\n",
    "        if os.path.isfile(os.path.join(target_dataset_name, image_target_filename))==False:\n",
    "            io.imsave(os.path.join(target_dataset_name, image_target_filename), image)\n",
    "\n",
    "\n",
    "        # Image height and width\n",
    "        height = int(xml_retreiver(root, 'nrows'))\n",
    "        width = int(xml_retreiver(root, 'ncols'))\n",
    "\n",
    "        assert height == image.shape[0]\n",
    "        assert width == image.shape[1]\n",
    "\n",
    "\n",
    "        # Miscellaneous metadata\n",
    "        date_captured = DateCaptured()\n",
    "        coco_url = 'n/a'\n",
    "        flickr_url = 'n/a'\n",
    "        license = np.random.randint(8)\n",
    "\n",
    "        # Appending to 'images'\n",
    "        minival2014['images'].append({'coco_url': coco_url, 'file_name': image_target_filename, \n",
    "                                     'date_captured': date_captured, 'flickr_url': flickr_url,\n",
    "                                     'height': height, 'id': image_id, 'license': license, \n",
    "                                     'width': width})\n",
    "        # Polygon Handling\n",
    "\n",
    "        for d, delete_flag in enumerate(delete_flags):\n",
    "            delete_flag = int(delete_flag.text)\n",
    "            if not delete_flag:\n",
    "\n",
    "\n",
    "                # ANNOTATIONS\n",
    "                # Bbox\n",
    "                bbox_x = int(xmins[d].text)\n",
    "                bbox_y = int(ymins[d].text)\n",
    "                bbox_w = int(xmaxs[d].text) - bbox_x\n",
    "                bbox_h = int(ymaxs[d].text) - bbox_y\n",
    "                bbox_list = [bbox_x, bbox_y, bbox_w, bbox_h]\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "#                 queries_findid = [d for d, image_nr in enumerate(image_nrs) if image_nr==image_filename[:-4]]\n",
    "#                 for query in queries_findid:\n",
    "#                     print(bbox_filiz[query])\n",
    "#                     print(bbox_list)\n",
    "#                 iou_for_each_xml_object = [iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid]\n",
    "#                 avg_iou.append(iou_calculator(bbox_filiz[query], bbox_list) for query in queries_findid)    \n",
    "#                 print(iou_for_each_xml_object)\n",
    "#                 print(\"\")\n",
    "\n",
    "\n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "\n",
    "#                 with open(\"bbox_info.txt\", \"a+\") as f:\n",
    "#                     f.write(image_filename[:-4]+\",\"+str(bbox_x)+\",\"+str(bbox_y)+\",\"+\n",
    "#                             str(bbox_x+bbox_w)+\",\"+str(bbox_y)+\",\"+\n",
    "#                             str(bbox_x)+\",\"+str(bbox_y+bbox_h)+\",\"+\n",
    "#                            str(bbox_x+bbox_w)+\",\"+str(bbox_y+bbox_h)+\"\\n\")\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "                # Class\n",
    "                category_id = dict_of_categories[class_name]\n",
    "\n",
    "                # Masks Segmentation\n",
    "                instance_seg_info = []\n",
    "                mask = cv2.imread(os.path.join(mask_dir, mask_filenames[d].text), 0)\n",
    "                polygons, binary_image = mask2poly(mask) \n",
    "                RLE_mask = binary_mask_to_rle(mask)\n",
    "                RLE_mask_coco = comask.encode(np.asfortranarray(binary_image))\n",
    "\n",
    "                # Area\n",
    "                area = comask.area(RLE_mask_coco)\n",
    "\n",
    "                #  Iscrowd\n",
    "                if (len(mask_filenames) - sum_deleted) > 1:\n",
    "                    iscrowd = 1\n",
    "                    minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                       'image_id': image_id, 'segmentation': RLE_mask,\n",
    "                                                       'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "                else:\n",
    "                    iscrowd = 0\n",
    "                    minival2014['annotations'].append({'iscrowd': iscrowd, 'bbox': bbox_list, 'id': annotation_id,\n",
    "                                                  'image_id': image_id, 'segmentation': polygons,\n",
    "                                                  'area': area, 'category_id': dict_of_categories[class_name]})\n",
    "\n",
    "                # Appending into annotations\n",
    "                \n",
    "                annotation_id+=1\n",
    "        image_id+=1\n",
    "\n",
    "                \n",
    "#                 print(bbox_x, bbox_y, bbox_w, bbox_h)\n",
    "# print(\"Average IoU = {}\".format(sum(avg_iou)/len(avg_iou)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in memory and then writes them into a file. \n",
    "\n",
    "# from decimal import Decimal\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "with open(\"Datasets/coco/annotations/instances_train2014.json\",\"w\") as f:\n",
    "# with open(\"Datasets/coco/annotations/instances_valminusminival2014.json\",\"w\") as f:\n",
    "    data = json.dumps(minival2014, cls=MyEncoder, indent=4)\n",
    "    f.write(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
